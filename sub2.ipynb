{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Configure matplotlib\n",
    "%matplotlib inline\n",
    "# Alternative: Use the notebook backend\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title: Real-Time 2D Object Detection with YOLOv8 on Pascal VOC Dataset\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Object detection is a fundamental task in computer vision with wide-ranging applications, from autonomous driving to surveillance and medical imaging. This project aims to implement and evaluate a state-of-the-art deep learning model, YOLOv8, for 2D object detection. We will utilize the well-established Pascal VOC 2007 dataset to train and test our model, focusing on achieving a balance between detection accuracy and inference speed. This notebook (and accompanying Medium post) will walk through the entire pipeline, from data selection and preprocessing to model training, evaluation, and result interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Objectives and Goals\n",
    "\n",
    "The primary objectives of this project are:\n",
    "1.  To understand and implement a modern object detection pipeline using a deep learning approach.\n",
    "2.  To gain hands-on experience with the YOLOv8 architecture, a popular and efficient object detection model.\n",
    "3.  To effectively preprocess the Pascal VOC 2007 dataset for training with YOLOv8.\n",
    "4.  To train the YOLOv8 model on the prepared dataset and evaluate its performance using standard object detection metrics.\n",
    "5.  To analyze the results and discuss the model's strengths, weaknesses, and potential areas for improvement.\n",
    "\n",
    "The goal is to develop a functional object detection system capable of identifying and localizing multiple objects within an image with reasonable accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection: Pascal VOC 2007\n",
    "\n",
    "For this project, we have selected the **Pascal Visual Object Classes (VOC) 2007 dataset**.\n",
    "\n",
    "**Source:** The dataset is publicly available and widely used in the computer vision community. It can typically be downloaded from the official Pascal VOC website or various academic mirrors.\n",
    "*   Official Site (historical): [http://host.robots.ox.ac.uk/pascal/VOC/voc2007/](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/)\n",
    "\n",
    "**Suitability:**\n",
    "*   **Benchmark Dataset:** VOC 2007 is a standard benchmark for object detection tasks, making it suitable for comparing our model's performance against established results.\n",
    "*   **Variety of Objects:** It contains 20 object classes, offering a diverse set of categories for detection (e.g., person, car, cat, dog, chair, bottle).\n",
    "*   **Realistic Scenarios:** The images depict realistic scenes with objects in various contexts, scales, and levels of occlusion.\n",
    "*   **Annotation Availability:** It provides detailed annotations, including bounding boxes and class labels for objects in the images, which are crucial for supervised training.\n",
    "*   **Manageable Size:** While comprehensive, its size is manageable for training on typical modern hardware within a reasonable timeframe.\n",
    "\n",
    "Our current setup uses the VOC 2007 data located within the `datasets/VOCdevkit/VOC2007/` directory in this project's workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "The Pascal VOC 2007 dataset consists of images and their corresponding annotations.\n",
    "\n",
    "*   **Data Format:**\n",
    "    *   **Images:** Primarily JPEG (`.jpg`) files.\n",
    "    *   **Annotations:** Originally provided in XML format (`.xml`), one file per image. Each XML file contains information about the image (filename, size) and details for each annotated object (class label, bounding box coordinates: xmin, ymin, xmax, ymax).\n",
    "\n",
    "*   **Dataset Size (VOC 2007 Main Competition):**\n",
    "    *   **Total Images:** Approximately 9,963 images.\n",
    "    *   **Training/Validation Images:** 5,011 images.\n",
    "    *   **Test Images:** 4,952 images.\n",
    "    *   **Object Classes:** 20 distinct classes.\n",
    "\n",
    "*   **Attributes (Object Classes):**\n",
    "    The 20 object classes are:\n",
    "    `aeroplane`, `bicycle`, `bird`, `boat`, `bottle`, `bus`, `car`, `cat`, `chair`, `cow`, `diningtable`, `dog`, `horse`, `motorbike`, `person`, `pottedplant`, `sheep`, `sofa`, `train`, `tvmonitor`.\n",
    "    *(Note: The `background` class is implicit and handled during training).*\n",
    "\n",
    "*   **Inherent Challenges or Limitations:**\n",
    "    *   **Class Imbalance:** Some object classes appear more frequently than others, which can bias the model.\n",
    "    *   **Object Scale Variation:** Objects appear at various sizes, from very small to large, posing a challenge for detection models.\n",
    "    *   **Occlusion:** Objects can be partially or heavily occluded by other objects.\n",
    "    *   **Intra-class Variation:** Objects within the same class can have significant visual differences.\n",
    "    *   **Clutter and Complex Scenes:** Images can be cluttered with multiple objects and complex backgrounds.\n",
    "    *   **Annotation Quality:** While generally good, there might be minor inconsistencies or inaccuracies in annotations.\n",
    "    *   **Dataset Size:** While used as a general benchmark, this dataset is rather small for deep learning tasks and will likely require data augmentation and/or transfer learning\n",
    "\n",
    "\n",
    "**Alignment with Project Objectives:**\n",
    "The VOC 2007 dataset directly aligns with our project objectives as it provides the necessary annotated image data to train and evaluate a supervised object detection model like YOLOv8. The variety of classes and challenging scenarios allow for a robust assessment of the model's capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing (5%)\n",
    "\n",
    "As outlined previously, the Pascal VOC 2007 dataset requires conversion to be compatible with YOLOv8. Our `train_yolov8_model.py` script (and the functions we'll show below, extracted from it) handles this. The key steps are:\n",
    "\n",
    "1.  **Annotation Conversion & Directory Structuring**: Handled by `convert_voc_to_yolo`.\n",
    "2.  **Dataset YAML Configuration**: Handled by `create_dataset_yaml`.\n",
    "\n",
    "Let's look at the Python functions responsible for these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4090 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import xml.etree.ElementTree as ET\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Convert VOC to YOLO Format\n",
    "\n",
    "# VOC class names (matching your VOC dataset)\n",
    "VOC_CLASSES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "\n",
    "def convert_voc_to_yolo(voc_path, output_path):\n",
    "    \"\"\"\n",
    "    Convert VOC format dataset to YOLO format\n",
    "    \n",
    "    Args:\n",
    "        voc_path: Path to the VOC dataset (e.g., datasets/VOCdevkit/VOC2007)\n",
    "        output_path: Path to save the converted YOLO format dataset\n",
    "    \n",
    "    Returns:\n",
    "        Path to the converted dataset\n",
    "    \"\"\"\n",
    "    print(f\"Converting VOC dataset from {voc_path} to YOLO format at {output_path}\")\n",
    "    \n",
    "    voc_path = Path(voc_path).absolute()\n",
    "    output_path = Path(output_path).absolute()\n",
    "    \n",
    "    # Create output directories\n",
    "    train_img_dir = output_path / 'train' / 'images'\n",
    "    train_label_dir = output_path / 'train' / 'labels'\n",
    "    val_img_dir = output_path / 'val' / 'images'\n",
    "    val_label_dir = output_path / 'val' / 'labels'\n",
    "    \n",
    "    # Clear existing data if any\n",
    "    if output_path.exists():\n",
    "        print(f\"Output directory {output_path} already exists. Removing existing data...\")\n",
    "        shutil.rmtree(output_path, ignore_errors=True)\n",
    "    \n",
    "    os.makedirs(train_img_dir, exist_ok=True)\n",
    "    os.makedirs(train_label_dir, exist_ok=True)\n",
    "    os.makedirs(val_img_dir, exist_ok=True)\n",
    "    os.makedirs(val_label_dir, exist_ok=True)\n",
    "    \n",
    "    # Get train/val split from ImageSets\n",
    "    train_ids = []\n",
    "    val_ids = []\n",
    "    \n",
    "    # Read trainval split\n",
    "    with open(voc_path / 'ImageSets' / 'Main' / 'trainval.txt', 'r') as f:\n",
    "        trainval_ids = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # If test.txt exists, use it for validation, otherwise split trainval\n",
    "    test_file = voc_path / 'ImageSets' / 'Main' / 'test.txt'\n",
    "    if test_file.exists():\n",
    "        with open(test_file, 'r') as f:\n",
    "            val_ids = [line.strip() for line in f if line.strip()]\n",
    "        train_ids = trainval_ids\n",
    "    else:\n",
    "        # Use 80% for training, 20% for validation\n",
    "        split_idx = int(len(trainval_ids) * 0.8)\n",
    "        train_ids = trainval_ids[:split_idx]\n",
    "        val_ids = trainval_ids[split_idx:]\n",
    "    \n",
    "    print(f\"Found {len(train_ids)} training images and {len(val_ids)} validation images\")\n",
    "    \n",
    "    # Convert annotations and copy images\n",
    "    def process_set(image_ids, img_output_dir, label_output_dir):\n",
    "        for img_id in tqdm(image_ids, desc=\"Converting annotations\"):\n",
    "            # Copy image\n",
    "            src_img = voc_path / 'JPEGImages' / f\"{img_id}.jpg\"\n",
    "            if not src_img.exists():  # Try png if jpg doesn't exist\n",
    "                src_img = voc_path / 'JPEGImages' / f\"{img_id}.png\"\n",
    "                if not src_img.exists():\n",
    "                    print(f\"Warning: Image {img_id} not found\")\n",
    "                    continue\n",
    "            \n",
    "            dst_img = img_output_dir / f\"{img_id}.jpg\"\n",
    "            shutil.copy(src_img, dst_img)\n",
    "            \n",
    "            # Convert annotation\n",
    "            anno_path = voc_path / 'Annotations' / f\"{img_id}.xml\"\n",
    "            if not anno_path.exists():\n",
    "                print(f\"Warning: Annotation for {img_id} not found\")\n",
    "                continue\n",
    "            \n",
    "            # Parse XML\n",
    "            tree = ET.parse(anno_path)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # Get image size\n",
    "            size = root.find('size')\n",
    "            img_width = int(size.find('width').text)\n",
    "            img_height = int(size.find('height').text)\n",
    "            \n",
    "            # Process objects\n",
    "            yolo_lines = []\n",
    "            for obj in root.findall('object'):\n",
    "                # Get class name\n",
    "                class_name = obj.find('name').text\n",
    "                if class_name in VOC_CLASSES:\n",
    "                    class_id = VOC_CLASSES.index(class_name) - 1  # Subtract 1 to remove background class\n",
    "                    if class_id < 0:  # Skip background\n",
    "                        continue\n",
    "                else:\n",
    "                    print(f\"Warning: Unknown class {class_name} in {img_id}\")\n",
    "                    continue\n",
    "                \n",
    "                # Get bounding box\n",
    "                bbox = obj.find('bndbox')\n",
    "                xmin = float(bbox.find('xmin').text)\n",
    "                ymin = float(bbox.find('ymin').text)\n",
    "                xmax = float(bbox.find('xmax').text)\n",
    "                ymax = float(bbox.find('ymax').text)\n",
    "                \n",
    "                # Convert to YOLO format: class_id x_center y_center width height\n",
    "                x_center = (xmin + xmax) / (2 * img_width)\n",
    "                y_center = (ymin + ymax) / (2 * img_height)\n",
    "                width = (xmax - xmin) / img_width\n",
    "                height = (ymax - ymin) / img_height\n",
    "                \n",
    "                # Add to YOLO format lines\n",
    "                yolo_lines.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "            \n",
    "            # Write YOLO format annotation\n",
    "            with open(label_output_dir / f\"{img_id}.txt\", 'w') as f:\n",
    "                f.write('\\n'.join(yolo_lines))\n",
    "    \n",
    "    # Process train and validation sets\n",
    "    process_set(train_ids, train_img_dir, train_label_dir)\n",
    "    process_set(val_ids, val_img_dir, val_label_dir)\n",
    "    \n",
    "    # Create class.txt file\n",
    "    with open(output_path / \"class.txt\", 'w') as f:\n",
    "        # Skip background class (index 0)\n",
    "        f.write('\\n'.join(VOC_CLASSES[1:]))\n",
    "    \n",
    "    print(\"Conversion completed!\")\n",
    "    return str(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Create Dataset YAML Config\n",
    "def create_dataset_yaml(dataset_path, name=\"voc_dataset\"):\n",
    "    \"\"\"\n",
    "    Create a YAML file for the dataset configuration\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path).absolute()\n",
    "    yaml_path = dataset_path / f\"{name}.yaml\"\n",
    "    \n",
    "    # Create YAML content with absolute paths\n",
    "    data = {\n",
    "        'path': str(dataset_path),\n",
    "        'train': str(dataset_path / 'train' / 'images'),\n",
    "        'val': str(dataset_path / 'val' / 'images'),\n",
    "        'test': str(dataset_path / 'val' / 'images'),  # Use val as test set too\n",
    "        'names': {}\n",
    "    }\n",
    "    \n",
    "    # Check if class.txt exists to define class names\n",
    "    class_file = dataset_path / \"class.txt\"\n",
    "    if class_file.exists():\n",
    "        with open(class_file, 'r') as f:\n",
    "            class_names = f.read().strip().split('\\n')\n",
    "            data['names'] = {i: name for i, name in enumerate(class_names)}\n",
    "    \n",
    "    # Write YAML file\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(data, f, sort_keys=False)\n",
    "    \n",
    "    print(f\"Created dataset config at {yaml_path}\")\n",
    "    return str(yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Implementation (10%)\n",
    "\n",
    "For this project, we selected the **YOLOv8 (You Only Look Once version 8)** model, a state-of-the-art, real-time object detection system developed by Ultralytics.\n",
    "\n",
    "### 6.1. Model Choice: YOLOv8\n",
    "\n",
    "**Rationale:**\n",
    "*   **Performance:** YOLOv8 offers an excellent balance of speed and accuracy, making it suitable for a wide range of applications, including those requiring real-time processing.\n",
    "*   **Ease of Use:** The Ultralytics framework provides a user-friendly Python API for training, validation, and inference with YOLOv8 models.\n",
    "*   **Scalability:** It comes in various sizes (nano `n`, small `s`, medium `m`, large `l`, extra-large `x`), allowing users to choose a model that best fits their computational resources and performance requirements. For this project, we primarily focus on `yolov8n.pt` (nano version) for faster training and iteration, demonstrating the pipeline.\n",
    "*   **Active Development:** YOLOv8 is actively maintained and updated with the latest advancements in object detection.\n",
    "\n",
    "### 6.2. Architecture Overview (Brief)\n",
    "\n",
    "YOLOv8 builds upon the successes of its predecessors. Key architectural features typically include:\n",
    "*   **Backbone:** A powerful feature extractor (e.g., based on CSPDarknet principles).\n",
    "*   **Neck:** A feature aggregation network (like PANet or BiFPN) that combines features from different scales to improve detection of objects of various sizes.\n",
    "*   **Head (Detection Head):** Responsible for predicting bounding boxes, class probabilities, and objectness scores. YOLOv8 uses an anchor-free, decoupled head, which can lead to faster NMS and improved accuracy.\n",
    "\n",
    "### 6.3. Loading the Model and Experiment Design\n",
    "\n",
    "The Ultralytics library makes loading YOLOv8 models straightforward. In this project, we implement four distinct experimental scenarios to thoroughly evaluate model performance:\n",
    "\n",
    "1. **Transfer Learning (Pre-trained)**: Loading a model pre-trained on COCO dataset and fine-tuning on our Pascal VOC dataset, using standard data augmentation.\n",
    "   ```python\n",
    "   model = YOLO(\"yolov8n.pt\")  # Load pre-trained weights\n",
    "   ```\n",
    "\n",
    "2. **Transfer Learning with Enhanced Augmentation**: Using pre-trained weights but applying stronger data augmentation techniques during training, including mosaic, mixup, and geometric transformations.\n",
    "   ```python\n",
    "   model = YOLO(\"yolov8n.pt\")  # Load pre-trained weights\n",
    "   # Apply enhanced augmentation during training\n",
    "   ```\n",
    "\n",
    "3. **Training from Scratch**: Training a model with the same architecture but with randomly initialized weights, allowing it to learn solely from our dataset.\n",
    "   ```python\n",
    "   model = YOLO(\"yolov8n.yaml\")  # Load only the model configuration\n",
    "   ```\n",
    "\n",
    "4. **Training from Scratch with Enhanced Augmentation**: Similar to approach #3, but with stronger data augmentation techniques to potentially improve generalization with limited data.\n",
    "   ```python\n",
    "   model = YOLO(\"yolov8n.yaml\")  # Load only the model configuration\n",
    "   # Apply enhanced augmentation during training\n",
    "   ```\n",
    "\n",
    "This comprehensive experimental design allows us to assess:\n",
    "- The impact of transfer learning vs. learning from scratch\n",
    "- The additional benefit of enhanced data augmentation\n",
    "- The interaction between pre-training and augmentation strategies\n",
    "\n",
    "Each model will be evaluated using the same metrics to enable direct comparisons across the four approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model Training Function\n",
    "def train_model(dataset_yaml, model_size='n', epochs=50, batch_size=16, img_size=640, \n",
    "                pretrained=True, project_name=\"yolo_training\"):\n",
    "    \"\"\"\n",
    "    Train a YOLOv8 model on a custom dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_yaml: str\n",
    "        Path to the dataset YAML configuration file\n",
    "    model_size: str\n",
    "        Size of YOLOv8 model ('n', 's', 'm', 'l', 'x')\n",
    "    epochs: int\n",
    "        Number of training epochs\n",
    "    batch_size: int\n",
    "        Batch size for training\n",
    "    img_size: int\n",
    "        Image size for training\n",
    "    pretrained: bool\n",
    "        Whether to start with pretrained weights (recommended)\n",
    "    project_name: str\n",
    "        Name for the project folder to save results\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model: YOLO\n",
    "        The trained YOLO model\n",
    "    \"\"\"\n",
    "    print(f\"Starting training with YOLOv8{model_size} for {epochs} epochs\")\n",
    "    \n",
    "    # Initialize model\n",
    "    if pretrained:\n",
    "        model = YOLO(f\"yolov8{model_size}.pt\")  # load pretrained model\n",
    "    else:\n",
    "        model = YOLO(f\"yolov8{model_size}.yaml\")  # create new model from scratch\n",
    "    \n",
    "    # Log dataset yaml content\n",
    "    print(f\"Using dataset config: {dataset_yaml}\")\n",
    "    with open(dataset_yaml, 'r') as f:\n",
    "        yaml_content = f.read()\n",
    "        print(f\"YAML Content:\\n{yaml_content}\")\n",
    "    \n",
    "    # Start training with absolute paths\n",
    "    results = model.train(\n",
    "        data=dataset_yaml,\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=img_size,\n",
    "        project=project_name,\n",
    "        name=f\"yolov8{model_size}_custom\",\n",
    "        patience=10,  # early stopping patience\n",
    "        save=True,  # save checkpoints\n",
    "        device=0 if torch.cuda.is_available() else 'cpu',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Model Validation Function\n",
    "def validate_model(model, dataset_yaml):\n",
    "    \"\"\"\n",
    "    Validate the trained model on validation dataset\n",
    "    \"\"\"\n",
    "    # Run validation\n",
    "    metrics = model.val(data=dataset_yaml, split='val', plots=True)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    if metrics and hasattr(metrics, 'box'):\n",
    "        print(f\"mAP50-95 (Box): {metrics.box.map:.4f}\")   # Mean Average Precision @ IoU=0.50:0.95\n",
    "        print(f\"mAP50 (Box):    {metrics.box.map50:.4f}\") # Mean Average Precision @ IoU=0.50\n",
    "        print(f\"mAP75 (Box):    {metrics.box.map75:.4f}\") # Mean Average Precision @ IoU=0.75\n",
    "        print(f\"Mean Precision (Box): {metrics.box.mp:.4f}\") # Mean Precision across all classes\n",
    "        print(f\"Mean Recall (Box):    {metrics.box.mr:.4f}\") # Mean Recall across all classes\n",
    "        \n",
    "        # For individual class metrics\n",
    "        print(\"\\nPer-class metrics (mAP50):\")\n",
    "        if hasattr(metrics.box, 'ap50'):\n",
    "            class_names = model.names\n",
    "            for i, ap in enumerate(metrics.box.ap50):\n",
    "                if i < len(class_names):\n",
    "                    print(f\"  {class_names[i]}: {ap:.4f}\")\n",
    "                    \n",
    "        print(f\"\\nValidation plots and results saved to: {metrics.save_dir}\")\n",
    "    else:\n",
    "        print(\"Validation metrics or box metrics not available.\")\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model Export and Results Plotting\n",
    "def export_model(model, format='onnx'):\n",
    "    \"\"\"\n",
    "    Export trained model to different formats for deployment\n",
    "    Supported formats: onnx, openvino, torchscript, tflite, etc.\n",
    "    \"\"\"\n",
    "    # Export the model\n",
    "    model.export(format=format)\n",
    "    print(f\"Model exported to {format} format\")\n",
    "\n",
    "def plot_training_results(results_path):\n",
    "    \"\"\"\n",
    "    Plot training results\n",
    "    \"\"\"\n",
    "    from ultralytics.utils.plotting import plot_results\n",
    "    \n",
    "    # Find the results.csv file\n",
    "    if os.path.isdir(results_path):\n",
    "        results_file = Path(results_path) / \"results.csv\"\n",
    "    else:\n",
    "        results_file = results_path\n",
    "    \n",
    "    # Plot results\n",
    "    fig, ax = plot_results(results_file)\n",
    "    plt.savefig(Path(results_path) / \"training_results.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Methods (5%)\n",
    "\n",
    "This section outlines the methodologies employed for training and evaluating our YOLOv8 object detection model on the prepared Pascal VOC 2007 dataset.\n",
    "\n",
    "### 7.1. Training Methodology\n",
    "\n",
    "The core of our method involves training the YOLOv8 model using the Ultralytics framework. We explore four distinct training strategies, creating a 2Ã—2 experimental design:\n",
    "\n",
    "1. **Transfer Learning (Fine-tuning):**\n",
    "   * **Initialization:** We start with a YOLOv8 model (e.g., `yolov8n.pt`) that has been pre-trained on a large-scale dataset like COCO.\n",
    "   * **Training Process:** The pre-trained model is then fine-tuned on our Pascal VOC 2007 dataset using standard augmentation.\n",
    "   * **Advantages:** Typically leads to faster convergence and better performance, especially with smaller custom datasets.\n",
    "\n",
    "2. **Transfer Learning with Enhanced Augmentation:**\n",
    "   * **Initialization:** Same as above, starting with COCO pre-trained weights.\n",
    "   * **Training Process:** We apply stronger data augmentation during training, including increased mosaic, mixup, geometric transformations, etc.\n",
    "   * **Hypothesis:** Enhanced augmentation may further improve performance by creating more diverse training samples.\n",
    "\n",
    "3. **Training from Scratch:**\n",
    "   * **Initialization:** We initialize a YOLOv8 model (via `yolov8n.yaml`) with random weights. The model architecture is defined, but it has no prior knowledge.\n",
    "   * **Training Process:** The model is trained solely on our Pascal VOC 2007 dataset with standard augmentation.\n",
    "   * **Purpose:** To understand the model's learning capacity on the specific dataset without influence from other data sources.\n",
    "\n",
    "4. **Training from Scratch with Enhanced Augmentation:**\n",
    "   * **Initialization:** Same as above, starting with random weights.\n",
    "   * **Training Process:** We apply the same enhanced augmentation as in strategy #2.\n",
    "   * **Hypothesis:** This may partially compensate for lack of pre-training by artificially increasing training data diversity.\n",
    "\n",
    "This 2Ã—2 experimental design allows us to isolate and measure:\n",
    "- The effect of transfer learning vs. learning from scratch\n",
    "- The impact of enhanced data augmentation\n",
    "- The interaction between these two factors (whether augmentation benefits pre-trained models differently than models trained from scratch)\n",
    "\n",
    "### 7.2. Evaluation Methodology\n",
    "\n",
    "Model performance is assessed using standard object detection metrics, primarily calculated on the validation set. The Ultralytics framework automatically computes these after training if `val=True` (default) or when the `.val()` method is explicitly called.\n",
    "\n",
    "**Key Evaluation Metrics:**\n",
    "*   **Intersection over Union (IoU):** A measure of the overlap between a predicted bounding box and a ground-truth bounding box.\n",
    "    \\[ IoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}} \\]\n",
    "*   **Precision:** The accuracy of positive predictions.\n",
    "    \\[ Precision = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Positives (FP)}} \\]\n",
    "    (TP: Correctly detected object; FP: Incorrectly detected object, or detection where no object exists)\n",
    "*   **Recall:** The ability of the model to find all relevant instances.\n",
    "    \\[ Recall = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Negatives (FN)}} \\]\n",
    "    (FN: A ground-truth object that the model failed to detect)\n",
    "*   **Average Precision (AP):** The area under the Precision-Recall curve for a specific class. It summarizes the model's performance for that class across all recall levels.\n",
    "*   **mean Average Precision (mAP):** The average of the AP values across all object classes. This is the primary metric for comparing object detection models.\n",
    "    *   **mAP@0.5 (or mAP50):** mAP calculated at a single IoU threshold of 0.5. This means a detection is considered a True Positive if its IoU with the ground truth is >= 0.5.\n",
    "    *   **mAP@0.5:0.95 (or mAP):** The average mAP over multiple IoU thresholds, typically from 0.5 to 0.95 with a step of 0.05 (COCO standard). This provides a more comprehensive evaluation across different levels of localization accuracy.\n",
    "\n",
    "Our `validate_model` function in `train_yolov8_model.py` is used to trigger the validation process and print these metrics.\n",
    "\n",
    "### 7.3. Software and Hardware\n",
    "*   **Primary Library:** Ultralytics YOLOv8\n",
    "*   **Deep Learning Framework:** PyTorch\n",
    "*   **Key Python Packages:** `torch`, `torchvision`, `ultralytics`, `pyyaml`, `matplotlib`, `numpy`, `Pillow`, `tqdm`\n",
    "*   **Development Environment:** Python 3.10, Jupyter Notebook\n",
    "*   **Hardware:** Training was performed using a system equipped with an NVIDIA GeForce RTX 4090 Laptop GPU. CPU was used for some preprocessing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiments and Results (10%)\n",
    "\n",
    "This section details the experiments conducted using our four-pronged approach to YOLOv8 model training on Pascal VOC 2007. We will report performance metrics and visualize results across all four experiments.\n",
    "\n",
    "**Setup:**\n",
    "* **Dataset:** Processed Pascal VOC 2007 (converted to YOLO format)\n",
    "* **Model:** YOLOv8n (nano version)\n",
    "* **Hardware:** NVIDIA GeForce RTX 4090 Laptop GPU\n",
    "* **Common Parameters:**\n",
    "  * Image Size (`imgsz`): 640\n",
    "  * Batch Size: 16\n",
    "  * Epochs: 50 for all experiments\n",
    "  * Early stopping patience: 10 epochs\n",
    "\n",
    "### 8.1. Experiment 1: Transfer Learning with Standard Augmentation\n",
    "\n",
    "In this experiment, we fine-tune a YOLOv8n model pre-trained on the COCO dataset using our Pascal VOC 2007 data with standard data augmentation parameters.\n",
    "\n",
    "**Parameters:**\n",
    "* Model Initialization: `yolov8n.pt` (pre-trained weights)\n",
    "* Augmentation: Default Ultralytics settings\n",
    "\n",
    "### 8.2. Experiment 2: Transfer Learning with Enhanced Augmentation\n",
    "\n",
    "Building on Experiment 1, we use the same pre-trained weights but apply stronger data augmentation techniques during training.\n",
    "\n",
    "**Parameters:**\n",
    "* Model Initialization: `yolov8n.pt` (pre-trained weights)\n",
    "* Enhanced Augmentation:\n",
    "  * Mosaic: 1.0 (probability)\n",
    "  * Mixup: 0.5 (probability)\n",
    "  * Rotation: Â±20.0 degrees\n",
    "  * Translation: 0.2\n",
    "  * Shear: 10.0\n",
    "  * And other enhancements (see code)\n",
    "\n",
    "### 8.3. Experiment 3: Training from Scratch with Standard Augmentation\n",
    "\n",
    "In this experiment, we train a YOLOv8n model with randomly initialized weights using only our Pascal VOC 2007 data with standard augmentation.\n",
    "\n",
    "**Parameters:**\n",
    "* Model Initialization: `yolov8n.yaml` (architecture only, random weights)\n",
    "* Augmentation: Default Ultralytics settings\n",
    "\n",
    "### 8.4. Experiment 4: Training from Scratch with Enhanced Augmentation\n",
    "\n",
    "In our final experiment, we combine training from scratch with the enhanced augmentation techniques used in Experiment 2.\n",
    "\n",
    "**Parameters:**\n",
    "* Model Initialization: `yolov8n.yaml` (architecture only, random weights)\n",
    "* Enhanced Augmentation: Same as Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters set for all experiments. Ready to begin processing.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Setup Paths and Parameters for All Experiments\n",
    "\n",
    "# Set paths for your directory structure using absolute paths\n",
    "workspace_dir = Path(os.path.dirname(os.path.abspath(\"__file__\")))  # Modified for notebook\n",
    "voc_path = workspace_dir / \"datasets\" / \"VOCdevkit\" / \"VOC2007\"\n",
    "yolo_dataset_path = workspace_dir / \"datasets\" / \"voc_yolo_converted\"\n",
    "\n",
    "# Check if VOC dataset exists\n",
    "if not os.path.exists(voc_path):\n",
    "    print(f\"VOC dataset not found at {voc_path}\")\n",
    "else:\n",
    "    # Set common training parameters for all experiments\n",
    "    model_size = 'n'  # nano model (options: 'n', 's', 'm', 'l', 'x')\n",
    "    epochs = 50\n",
    "    batch_size = 16\n",
    "    img_size = 640\n",
    "    project_name = \"yolo_training\"\n",
    "    \n",
    "    # Define experiment variations\n",
    "    transfer_learning = True        # For experiments 1 & 2\n",
    "    enhanced_augmentation = False   # For experiments 2 & 4\n",
    "    \n",
    "    print(\"Parameters set for all experiments. Ready to begin processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Define Training Functions with Enhanced Augmentation\n",
    "\n",
    "# Function for transfer learning with enhanced augmentation\n",
    "def train_model_augmented(dataset_yaml, model_size='n', epochs=50, batch_size=16, img_size=640, \n",
    "                         project_name=\"yolo_training\", pretrained=True):\n",
    "    \"\"\"\n",
    "    Train a YOLOv8 model with enhanced data augmentation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pretrained: bool\n",
    "        Whether to use transfer learning (True) or train from scratch (False)\n",
    "    \"\"\"\n",
    "    train_type = \"transfer learning\" if pretrained else \"scratch training\"\n",
    "    print(f\"Starting {train_type} with YOLOv8{model_size} with enhanced augmentation for {epochs} epochs\")\n",
    "    \n",
    "    # Initialize model based on whether we're doing transfer learning or from-scratch\n",
    "    if pretrained:\n",
    "        model = YOLO(f\"yolov8{model_size}.pt\")  # Load pretrained model\n",
    "    else:\n",
    "        model = YOLO(f\"yolov8{model_size}.yaml\")  # Create model from scratch\n",
    "    \n",
    "    # Name for the experiment\n",
    "    exp_name = f\"yolov8{model_size}_{'pretrained' if pretrained else 'scratch'}_augmented\"\n",
    "    \n",
    "    # Start training with enhanced augmentation\n",
    "    results = model.train(\n",
    "        data=dataset_yaml,\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=img_size,\n",
    "        project=project_name,\n",
    "        name=exp_name,\n",
    "        patience=10,\n",
    "        save=True,\n",
    "        device=0 if torch.cuda.is_available() else 'cpu',\n",
    "        verbose=True,\n",
    "        # Enhanced augmentation parameters\n",
    "        mosaic=1.0,           # Increase mosaic probability (default is 1.0)\n",
    "        mixup=0.5,            # Add mixup augmentation (default is 0.0)\n",
    "        degrees=20.0,         # Rotation augmentation (default is 0.0)\n",
    "        translate=0.2,        # Translation augmentation (default is 0.1)\n",
    "        scale=0.2,            # Scale augmentation (default is 0.5)\n",
    "        shear=10.0,           # Shear augmentation (default is 0.0)\n",
    "        perspective=0.001,    # Perspective augmentation (default is 0.0)\n",
    "        flipud=0.1,           # Vertical flip (default is 0.0)\n",
    "        fliplr=0.5,           # Horizontal flip (default is 0.5)\n",
    "        hsv_h=0.015,          # HSV-Hue augmentation (default is 0.015)\n",
    "        hsv_s=0.7,            # HSV-Saturation augmentation (default is 0.7)\n",
    "        hsv_v=0.4,            # HSV-Value augmentation (default is 0.4)\n",
    "        copy_paste=0.1        # Copy-paste augmentation (default is 0.0)\n",
    "    )\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting VOC dataset from /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/VOCdevkit/VOC2007 to YOLO format at /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted\n",
      "Output directory /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted already exists. Removing existing data...\n",
      "Found 5011 training images and 4952 validation images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5011/5011 [05:04<00:00, 16.43it/s]\n",
      "Converting annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [05:02<00:00, 16.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed!\n",
      "Created dataset config at /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/voc_yolo.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Data Conversion (Run this cell to convert VOC to YOLO format)\n",
    "# This cell may take some time to execute\n",
    "converted_dataset_path = convert_voc_to_yolo(voc_path, yolo_dataset_path)\n",
    "dataset_yaml = create_dataset_yaml(converted_dataset_path, name=\"voc_yolo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPERIMENT 1: TRANSFER LEARNING (STANDARD) ===\n",
      "\n",
      "Starting training with YOLOv8n for 50 epochs\n",
      "Using dataset config: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/voc_yolo.yaml\n",
      "YAML Content:\n",
      "path: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted\n",
      "train: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/train/images\n",
      "val: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/images\n",
      "test: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/images\n",
      "names:\n",
      "  0: aeroplane\n",
      "  1: bicycle\n",
      "  2: bird\n",
      "  3: boat\n",
      "  4: bottle\n",
      "  5: bus\n",
      "  6: car\n",
      "  7: cat\n",
      "  8: chair\n",
      "  9: cow\n",
      "  10: diningtable\n",
      "  11: dog\n",
      "  12: horse\n",
      "  13: motorbike\n",
      "  14: person\n",
      "  15: pottedplant\n",
      "  16: sheep\n",
      "  17: sofa\n",
      "  18: train\n",
      "  19: tvmonitor\n",
      "\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/voc_yolo.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_custom3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=yolo_training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=yolo_training/yolov8n_custom3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=20\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,014,748 parameters, 3,014,732 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 2.1Â±0.3 ms, read: 24.5Â±6.6 MB/s, size: 94.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/train/labels... 5011 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5011/5011 [00:09<00:00, 511.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 3.5Â±0.8 ms, read: 16.9Â±5.7 MB/s, size: 86.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:08<00:00, 577.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache\n",
      "Plotting labels to yolo_training/yolov8n_custom3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolo_training/yolov8n_custom3\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50       2.1G      1.086      2.779      1.282         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:42<00:00,  7.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.663      0.591      0.629      0.419\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      2.56G      1.134      1.875      1.324         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.64      0.567       0.61      0.398\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50      2.57G      1.158      1.793      1.338         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.61      0.552      0.577      0.366\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50      2.57G       1.16      1.743      1.342         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.657      0.554      0.606      0.389\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50      2.58G      1.153      1.656      1.333         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.654       0.56      0.602      0.384\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50      2.58G      1.137      1.592      1.328         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.676      0.567      0.618      0.397\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50      2.58G      1.128      1.534      1.315         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.683      0.564      0.622      0.404\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50      2.71G      1.123      1.496      1.314         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.693      0.587      0.641      0.416\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50      2.71G      1.109      1.433      1.297         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976        0.7      0.603      0.658      0.433\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50      2.71G      1.096      1.404      1.292          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.699      0.585       0.65      0.433\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50      2.71G      1.087      1.365      1.286         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.693      0.596       0.65       0.43\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50      2.71G      1.084      1.331      1.277         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.701      0.582      0.642      0.427\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50      2.71G      1.078      1.318      1.275         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.716      0.611       0.67      0.447\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50      2.71G      1.051      1.273      1.265          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.709      0.616      0.673      0.447\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50      2.71G      1.056       1.26       1.26         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.72      0.618       0.68      0.457\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50      2.71G       1.04      1.231      1.255         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.727       0.62      0.682      0.458\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50      2.71G      1.043      1.208      1.253         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.738      0.608      0.682      0.463\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50      2.71G      1.025      1.187      1.242         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.729       0.62      0.685      0.463\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50      2.71G      1.025      1.165      1.235         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.737      0.625      0.697      0.471\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50      2.71G      1.015      1.135      1.234         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.744      0.623        0.7      0.473\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50      2.71G      1.008      1.132      1.227          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:40<00:00,  7.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.742      0.627      0.698      0.474\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50      2.71G     0.9908      1.105      1.223         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.756      0.628      0.702      0.478\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50      2.71G     0.9962      1.088      1.221         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.75      0.632      0.704      0.479\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50      2.71G     0.9962       1.08      1.217         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.736      0.637      0.706      0.481\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50      2.71G     0.9892      1.064      1.214         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.756      0.637      0.708      0.484\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50      2.71G     0.9788      1.057      1.206         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.745      0.641      0.711      0.484\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50      2.71G     0.9736      1.038      1.203         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.748      0.646       0.71      0.486\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50      2.71G     0.9745      1.024      1.203         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:33<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.762      0.632      0.712      0.488\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50      2.71G     0.9679      1.016      1.201         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:55<00:00,  5.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.762       0.65       0.72      0.498\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50      2.71G     0.9527      1.002      1.193         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.753      0.655      0.719      0.493\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50      2.71G     0.9517      0.973      1.189         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:33<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.784       0.64      0.725      0.497\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50      2.71G     0.9399     0.9794      1.185         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.767      0.656      0.727      0.502\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50      2.71G     0.9377     0.9609      1.183         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.772      0.648      0.724      0.498\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50      2.71G     0.9259     0.9518      1.175         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.775      0.659      0.734      0.511\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50      2.71G     0.9298     0.9467      1.174         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.776      0.661      0.735      0.513\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50      2.71G     0.9194     0.9237      1.165         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.773      0.664      0.732      0.509\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50      2.84G     0.9093     0.9143      1.164          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.768      0.666      0.735      0.512\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50      2.84G     0.9123     0.9064      1.163         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.785      0.657      0.735      0.511\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50      2.84G     0.9037     0.8929      1.152         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.768      0.666      0.737      0.514\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50      2.84G     0.9048     0.8868      1.151         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.77       0.67      0.739      0.515\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50      2.84G     0.8812     0.7748      1.134          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.776      0.656      0.729      0.505\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50      2.84G      0.858     0.7251      1.114          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.777      0.663      0.734       0.51\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50      2.84G     0.8461     0.7101      1.109          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:36<00:00,  8.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.781      0.662      0.733      0.512\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50      2.84G     0.8344     0.6896      1.102         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.775       0.67      0.736      0.513\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50      2.84G     0.8281     0.6753      1.097         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:33<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.774      0.673      0.742      0.521\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50      2.84G     0.8264     0.6797        1.1         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:36<00:00,  8.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.781      0.668      0.739       0.52\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50      2.84G     0.8194     0.6675      1.089         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:33<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.786       0.67      0.744      0.521\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50      2.84G     0.8181     0.6578      1.092         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.789      0.665      0.741       0.52\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50      2.84G     0.8034     0.6415      1.076          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:37<00:00,  8.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.79      0.666      0.744      0.523\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50      2.84G     0.8091     0.6438      1.088          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:36<00:00,  8.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.783      0.669      0.743      0.523\n",
      "\n",
      "50 epochs completed in 1.032 hours.\n",
      "Optimizer stripped from yolo_training/yolov8n_custom3/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from yolo_training/yolov8n_custom3/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating yolo_training/yolov8n_custom3/weights/best.pt...\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:41<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.788      0.667      0.744      0.523\n",
      "             aeroplane        205        311      0.816      0.726      0.811      0.558\n",
      "               bicycle        250        389      0.826      0.725      0.807      0.589\n",
      "                  bird        289        576      0.757      0.631      0.705      0.447\n",
      "                  boat        176        393      0.675      0.552      0.615      0.357\n",
      "                bottle        240        657      0.778      0.461      0.555      0.358\n",
      "                   bus        183        254      0.787      0.715      0.797       0.66\n",
      "                   car        775       1541      0.877       0.77      0.858      0.636\n",
      "                   cat        332        370      0.809      0.735      0.803      0.598\n",
      "                 chair        545       1374      0.749      0.463      0.574      0.366\n",
      "                   cow        127        329      0.781      0.751      0.806      0.569\n",
      "           diningtable        247        299      0.796      0.599      0.672      0.483\n",
      "                   dog        433        530      0.794        0.7      0.786      0.573\n",
      "                 horse        279        395      0.897      0.797      0.883      0.649\n",
      "             motorbike        233        369       0.85      0.721       0.84      0.572\n",
      "                person       2097       5227      0.856       0.77      0.855      0.576\n",
      "           pottedplant        254        592      0.672      0.463      0.516       0.29\n",
      "                 sheep         98        311       0.73      0.659       0.71      0.508\n",
      "                  sofa        355        396      0.665      0.631      0.698       0.53\n",
      "                 train        259        302      0.826      0.788      0.845      0.606\n",
      "             tvmonitor        255        361      0.829      0.683      0.748      0.537\n",
      "Speed: 0.3ms preprocess, 1.0ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1myolo_training/yolov8n_custom3\u001b[0m\n",
      "Transfer learning results saved to yolo_training/yolov8n_custom3\n",
      "\n",
      "Validating transfer learning model...\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 1.9Â±0.2 ms, read: 22.8Â±7.7 MB/s, size: 106.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:46<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.793      0.663      0.745      0.524\n",
      "             aeroplane        205        311      0.815       0.72      0.811      0.561\n",
      "               bicycle        250        389      0.826      0.721      0.808      0.588\n",
      "                  bird        289        576      0.763      0.627      0.705      0.447\n",
      "                  boat        176        393       0.68      0.547      0.614      0.357\n",
      "                bottle        240        657      0.794       0.46      0.557      0.361\n",
      "                   bus        183        254      0.787      0.713      0.795       0.66\n",
      "                   car        775       1541      0.877      0.767      0.859      0.636\n",
      "                   cat        332        370      0.823      0.732      0.807      0.601\n",
      "                 chair        545       1374      0.755      0.456      0.574      0.367\n",
      "                   cow        127        329      0.789      0.748      0.806      0.569\n",
      "           diningtable        247        299      0.798      0.596      0.672      0.484\n",
      "                   dog        433        530      0.796      0.694      0.785      0.572\n",
      "                 horse        279        395      0.901      0.797      0.883      0.651\n",
      "             motorbike        233        369      0.854      0.715      0.839      0.573\n",
      "                person       2097       5227      0.861      0.767      0.855      0.577\n",
      "           pottedplant        254        592      0.684      0.463      0.518      0.291\n",
      "                 sheep         98        311      0.737      0.659      0.709      0.506\n",
      "                  sofa        355        396      0.668       0.63        0.7      0.531\n",
      "                 train        259        302       0.83      0.781      0.844      0.604\n",
      "             tvmonitor        255        361      0.827      0.674      0.749      0.537\n",
      "Speed: 0.3ms preprocess, 1.8ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
      "Results saved to \u001b[1myolo_training/yolov8n_custom32\u001b[0m\n",
      "\n",
      "Validation Metrics:\n",
      "mAP50-95 (Box): 0.5236\n",
      "mAP50 (Box):    0.7445\n",
      "mAP75 (Box):    0.5716\n",
      "Mean Precision (Box): 0.7933\n",
      "Mean Recall (Box):    0.6634\n",
      "\n",
      "Per-class metrics (mAP50):\n",
      "  aeroplane: 0.8115\n",
      "  bicycle: 0.8081\n",
      "  bird: 0.7046\n",
      "  boat: 0.6140\n",
      "  bottle: 0.5575\n",
      "  bus: 0.7955\n",
      "  car: 0.8586\n",
      "  cat: 0.8068\n",
      "  chair: 0.5736\n",
      "  cow: 0.8057\n",
      "  diningtable: 0.6723\n",
      "  dog: 0.7847\n",
      "  horse: 0.8833\n",
      "  motorbike: 0.8391\n",
      "  person: 0.8552\n",
      "  pottedplant: 0.5177\n",
      "  sheep: 0.7090\n",
      "  sofa: 0.7004\n",
      "  train: 0.8441\n",
      "  tvmonitor: 0.7492\n",
      "\n",
      "Validation plots and results saved to: yolo_training/yolov8n_custom32\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Experiment 1 - Transfer Learning (Standard)\n",
    "print(\"\\n=== EXPERIMENT 1: TRANSFER LEARNING (STANDARD) ===\\n\")\n",
    "\n",
    "model_tl, results_tl = train_model(\n",
    "    dataset_yaml=dataset_yaml,\n",
    "    model_size=model_size,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size,\n",
    "    pretrained=True,  # Use transfer learning\n",
    "    project_name=project_name\n",
    ")\n",
    "\n",
    "# Get the results directory\n",
    "results_dir_tl = results_tl.save_dir\n",
    "print(f\"Transfer learning results saved to {results_dir_tl}\")\n",
    "\n",
    "# Validate the model\n",
    "print(\"\\nValidating transfer learning model...\")\n",
    "metrics_tl = validate_model(model_tl, dataset_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPERIMENT 2: TRANSFER LEARNING WITH ENHANCED AUGMENTATION ===\n",
      "\n",
      "Starting transfer learning with YOLOv8n with enhanced augmentation for 50 epochs\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.1, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/voc_yolo.yaml, degrees=20.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.1, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.5, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_pretrained_augmented4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.001, plots=True, pose=12.0, pretrained=True, profile=False, project=yolo_training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=yolo_training/yolov8n_pretrained_augmented4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.2, seed=0, shear=10.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.2, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=20\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,014,748 parameters, 3,014,732 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 2.9Â±1.2 ms, read: 18.5Â±6.1 MB/s, size: 94.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/train/labels... 5011 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5011/5011 [00:09<00:00, 549.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.8Â±0.4 ms, read: 16.5Â±5.0 MB/s, size: 86.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:08<00:00, 609.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache\n",
      "Plotting labels to yolo_training/yolov8n_pretrained_augmented4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolo_training/yolov8n_pretrained_augmented4\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      2.38G      1.801       3.42      1.946         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:48<00:00,  6.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:44<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.546      0.506      0.513      0.244\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      2.43G      1.752      2.732       1.94         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:51<00:00,  6.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:41<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.604       0.51      0.537      0.254\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50      2.59G       1.73      2.675      1.936         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:52<00:00,  5.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:40<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.574      0.486      0.503      0.236\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50      2.59G      1.729      2.611      1.942         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:39<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.593       0.51      0.538      0.254\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50      2.59G      1.715      2.561      1.922         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:39<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.591      0.488      0.524      0.267\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50      2.59G      1.693      2.498      1.906         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:46<00:00,  6.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.598      0.529      0.551      0.281\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50      2.59G      1.674      2.465      1.895         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:45<00:00,  6.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.623      0.521      0.552      0.269\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50      2.59G      1.662       2.42      1.883         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:41<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.641      0.531      0.576      0.311\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50      2.59G      1.657      2.381      1.876         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.633      0.544      0.575      0.301\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50      2.59G      1.645      2.374       1.87         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:45<00:00,  6.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.655      0.527      0.582      0.286\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50      2.59G      1.635      2.349      1.859         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.664      0.556      0.607      0.316\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50      2.59G      1.626      2.325      1.856         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:40<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.629      0.546      0.584       0.31\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50      2.59G      1.618      2.325      1.852         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.679      0.565      0.622      0.335\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50      2.59G       1.62      2.269      1.844         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.686      0.555      0.615      0.342\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50      2.59G      1.605      2.264      1.838         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.687      0.557      0.618      0.332\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50      2.59G      1.606      2.237      1.833         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.669      0.577      0.633      0.358\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50      2.59G        1.6      2.232      1.832         43        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.687      0.579      0.634      0.369\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50      2.59G      1.583      2.198      1.816         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.669      0.585      0.627      0.358\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50      2.59G      1.584      2.186      1.816         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976        0.7      0.583      0.644      0.367\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50      2.59G      1.588      2.192      1.813         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:34<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976        0.7      0.571       0.64      0.374\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50      2.59G      1.584      2.162      1.806         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:46<00:00,  6.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.708      0.589      0.657      0.378\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50      2.59G      1.571      2.162      1.806         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [01:21<00:00,  3.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.689      0.598      0.652      0.384\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50      2.59G      1.571      2.161      1.804         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:45<00:00,  6.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.714      0.607      0.665      0.399\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50      2.59G      1.566      2.133      1.796         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.693      0.594      0.655      0.383\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50      2.59G      1.553      2.114       1.79          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:46<00:00,  6.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.718      0.599      0.671      0.399\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50      2.59G      1.557       2.11      1.791         48        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:46<00:00,  6.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.721      0.607      0.673        0.4\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50      2.59G      1.551      2.086      1.779         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.71      0.608      0.668      0.388\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50      2.59G      1.553      2.084      1.779         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:46<00:00,  6.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.729      0.607      0.671      0.399\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50      2.59G      1.546      2.076      1.779         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:46<00:00,  6.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.716      0.613      0.673      0.395\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50      2.59G      1.546      2.078      1.784         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:45<00:00,  6.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:40<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.723      0.615      0.679      0.407\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50      2.59G      1.535      2.058       1.77         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:46<00:00,  6.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.727      0.613      0.678      0.398\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50      2.59G      1.527      2.033       1.76         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:45<00:00,  6.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.734      0.614      0.681      0.416\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50      2.59G      1.541      2.029      1.762         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:46<00:00,  6.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.724      0.615       0.68      0.414\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50      2.59G      1.526      2.021      1.757         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.729      0.618      0.684      0.405\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50      2.59G      1.515      1.996      1.752         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.734      0.623      0.691      0.421\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50      2.59G      1.515       2.01       1.76         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.716      0.629      0.689      0.426\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50      2.59G       1.52      2.013      1.751         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.744       0.62      0.691       0.42\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50      2.59G      1.512      1.982      1.749         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.747      0.622      0.697      0.427\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50      2.59G      1.504      1.965      1.741         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:47<00:00,  6.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.747      0.627      0.698      0.433\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50      2.59G        1.5      1.969      1.741         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  6.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:41<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.745      0.632        0.7      0.433\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50      2.59G       1.27      1.434      1.546          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:45<00:00,  6.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.756      0.614      0.695      0.427\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50      2.59G       1.25      1.276      1.524          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:40<00:00,  7.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:35<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.754      0.629      0.705      0.434\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50      2.59G      1.232      1.241       1.51          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:41<00:00,  7.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.744      0.637      0.703      0.439\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50      2.59G      1.237      1.217      1.511         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:40<00:00,  7.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:40<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.768      0.629      0.704      0.439\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50      2.59G      1.223      1.208      1.498         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:41<00:00,  7.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.75      0.642      0.711      0.448\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50      2.59G      1.205      1.174      1.487         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:40<00:00,  7.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:39<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.764      0.639       0.71       0.45\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50      2.59G       1.21      1.151      1.482         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:40<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.763       0.64      0.712      0.451\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50      2.59G      1.199      1.137      1.472          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:41<00:00,  7.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.758      0.651      0.715      0.452\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50      2.59G      1.189      1.117      1.462          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:41<00:00,  7.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.77      0.643      0.718      0.457\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50      2.59G      1.193      1.121       1.47          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:42<00:00,  7.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.763      0.646      0.718      0.459\n",
      "\n",
      "50 epochs completed in 1.185 hours.\n",
      "Optimizer stripped from yolo_training/yolov8n_pretrained_augmented4/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from yolo_training/yolov8n_pretrained_augmented4/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating yolo_training/yolov8n_pretrained_augmented4/weights/best.pt...\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:48<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.763      0.645      0.718      0.459\n",
      "             aeroplane        205        311      0.834      0.713      0.803      0.478\n",
      "               bicycle        250        389      0.834      0.736      0.805      0.539\n",
      "                  bird        289        576      0.801       0.62      0.704      0.396\n",
      "                  boat        176        393      0.673      0.471      0.535      0.287\n",
      "                bottle        240        657      0.743      0.413       0.53      0.295\n",
      "                   bus        183        254      0.806      0.688      0.775      0.586\n",
      "                   car        775       1541      0.858       0.74      0.836      0.575\n",
      "                   cat        332        370      0.753      0.765      0.805      0.537\n",
      "                 chair        545       1374      0.727      0.399      0.524        0.3\n",
      "                   cow        127        329      0.763      0.713      0.778      0.491\n",
      "           diningtable        247        299      0.723       0.55      0.642       0.41\n",
      "                   dog        433        530      0.767      0.704      0.771       0.51\n",
      "                 horse        279        395      0.867      0.773      0.858      0.576\n",
      "             motorbike        233        369      0.843      0.718      0.816      0.525\n",
      "                person       2097       5227      0.836       0.75      0.841      0.514\n",
      "           pottedplant        254        592      0.592      0.424      0.459      0.228\n",
      "                 sheep         98        311      0.696      0.691      0.724      0.475\n",
      "                  sofa        355        396      0.588      0.644      0.637      0.445\n",
      "                 train        259        302      0.784      0.755      0.781       0.51\n",
      "             tvmonitor        255        361      0.776      0.641      0.727      0.495\n",
      "Speed: 0.3ms preprocess, 1.1ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
      "Results saved to \u001b[1myolo_training/yolov8n_pretrained_augmented4\u001b[0m\n",
      "Transfer learning with augmentation results saved to yolo_training/yolov8n_pretrained_augmented4\n",
      "\n",
      "Validating transfer learning model with augmentation...\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.1Â±0.3 ms, read: 24.8Â±6.1 MB/s, size: 106.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 11: Experiment 2 - Transfer Learning with Enhanced Augmentation\n",
    "print(\"\\n=== EXPERIMENT 2: TRANSFER LEARNING WITH ENHANCED AUGMENTATION ===\\n\")\n",
    "\n",
    "model_tl_aug, results_tl_aug = train_model_augmented(\n",
    "    dataset_yaml=dataset_yaml,\n",
    "    model_size=model_size,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size,\n",
    "    pretrained=True,  # Use transfer learning\n",
    "    project_name=project_name\n",
    ")\n",
    "\n",
    "# Get the results directory\n",
    "results_dir_tl_aug = results_tl_aug.save_dir\n",
    "print(f\"Transfer learning with augmentation results saved to {results_dir_tl_aug}\")\n",
    "\n",
    "# Validate the model\n",
    "print(\"\\nValidating transfer learning model with augmentation...\")\n",
    "metrics_tl_aug = validate_model(model_tl_aug, dataset_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.7Â±0.8 ms, read: 21.2Â±5.1 MB/s, size: 87.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:09<00:00, 546.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:43<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.763      0.645      0.717      0.459\n",
      "             aeroplane        205        311      0.838      0.717      0.804      0.479\n",
      "               bicycle        250        389      0.831      0.735      0.805      0.541\n",
      "                  bird        289        576      0.801       0.62      0.703      0.396\n",
      "                  boat        176        393      0.679      0.473      0.534      0.287\n",
      "                bottle        240        657      0.739      0.409      0.529      0.294\n",
      "                   bus        183        254      0.806      0.687      0.773      0.584\n",
      "                   car        775       1541       0.86      0.741      0.836      0.575\n",
      "                   cat        332        370      0.758      0.765      0.807       0.54\n",
      "                 chair        545       1374      0.731      0.399      0.525        0.3\n",
      "                   cow        127        329      0.763      0.714      0.778      0.491\n",
      "           diningtable        247        299      0.723      0.549      0.643       0.41\n",
      "                   dog        433        530      0.762      0.702      0.769      0.509\n",
      "                 horse        279        395      0.867      0.776      0.858      0.579\n",
      "             motorbike        233        369      0.841      0.715      0.813      0.525\n",
      "                person       2097       5227      0.836       0.75      0.841      0.514\n",
      "           pottedplant        254        592      0.593      0.422      0.457      0.227\n",
      "                 sheep         98        311      0.691      0.688      0.721      0.474\n",
      "                  sofa        355        396      0.589      0.644      0.636      0.445\n",
      "                 train        259        302      0.785      0.758      0.782      0.511\n",
      "             tvmonitor        255        361      0.774       0.64      0.726      0.495\n",
      "Speed: 0.3ms preprocess, 1.7ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val5\u001b[0m\n",
      "mAP50: 0.7170\n",
      "mAP50-95: 0.4589\n",
      "\n",
      "Per-class metrics (mAP50):\n",
      "  aeroplane: 0.8039\n",
      "  bicycle: 0.8051\n",
      "  bird: 0.7034\n",
      "  boat: 0.5340\n",
      "  bottle: 0.5288\n",
      "  bus: 0.7727\n",
      "  car: 0.8360\n",
      "  cat: 0.8069\n",
      "  chair: 0.5249\n",
      "  cow: 0.7778\n",
      "  diningtable: 0.6425\n",
      "  dog: 0.7690\n",
      "  horse: 0.8580\n",
      "  motorbike: 0.8131\n",
      "  person: 0.8410\n",
      "  pottedplant: 0.4575\n",
      "  sheep: 0.7212\n",
      "  sofa: 0.6364\n",
      "  train: 0.7819\n",
      "  tvmonitor: 0.7262\n"
     ]
    }
   ],
   "source": [
    "#the above kernel crashed on validation so lets access the best weights and validate them\n",
    "# Cell: Validate the model with the best weights\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Path to the saved weights\n",
    "weights_path = \"yolo_training/yolov8n_pretrained_augmented4/weights/best.pt\"\n",
    "\n",
    "# Load the model\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# Validate the model on the validation dataset\n",
    "results = model.val(data=\"datasets/voc_yolo_converted/voc_yolo.yaml\")\n",
    "\n",
    "# Print validation results\n",
    "print(f\"mAP50: {results.box.map50:.4f}\")\n",
    "print(f\"mAP50-95: {results.box.map:.4f}\")\n",
    "\n",
    "# Print per-class metrics (mAP50)\n",
    "print(\"\\nPer-class metrics (mAP50):\")\n",
    "class_names = model.names\n",
    "for i, ap in enumerate(results.box.ap50):\n",
    "    print(f\"  {class_names[i]}: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPERIMENT 3: TRAINING FROM SCRATCH ===\n",
      "\n",
      "Starting training with YOLOv8n for 50 epochs\n",
      "Using dataset config: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/voc_yolo.yaml\n",
      "YAML Content:\n",
      "path: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted\n",
      "train: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/train/images\n",
      "val: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/images\n",
      "test: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/images\n",
      "names:\n",
      "  0: aeroplane\n",
      "  1: bicycle\n",
      "  2: bird\n",
      "  3: boat\n",
      "  4: bottle\n",
      "  5: bus\n",
      "  6: car\n",
      "  7: cat\n",
      "  8: chair\n",
      "  9: cow\n",
      "  10: diningtable\n",
      "  11: dog\n",
      "  12: horse\n",
      "  13: motorbike\n",
      "  14: person\n",
      "  15: pottedplant\n",
      "  16: sheep\n",
      "  17: sofa\n",
      "  18: train\n",
      "  19: tvmonitor\n",
      "\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/voc_yolo.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.yaml, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_custom5, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=yolo_training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=yolo_training/yolov8n_custom5, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=20\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
      "YOLOv8n summary: 129 layers, 3,014,748 parameters, 3,014,732 gradients, 8.2 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 2.2Â±0.4 ms, read: 18.1Â±6.1 MB/s, size: 94.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/train/labels... 5011 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5011/5011 [00:09<00:00, 551.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.5Â±0.2 ms, read: 16.5Â±5.1 MB/s, size: 86.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:08<00:00, 550.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache\n",
      "Plotting labels to yolo_training/yolov8n_custom5/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolo_training/yolov8n_custom5\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      2.16G      3.367      4.854      4.102         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976    0.00238      0.143    0.00221   0.000592\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      2.63G      2.989      4.418      3.506         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:41<00:00,  7.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976    0.00565      0.242    0.00645    0.00187\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50      2.64G      2.493      4.085      2.947         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.626      0.019     0.0147    0.00566\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50      2.64G      2.199      3.774      2.611         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.268     0.0603     0.0296     0.0115\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50      2.65G      2.063      3.568      2.448         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.325     0.0655     0.0404      0.017\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50      2.65G      1.966      3.389      2.346         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.256      0.109     0.0621     0.0273\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50      2.65G      1.913      3.279      2.283         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:40<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.208      0.152     0.0569     0.0251\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50      2.77G      1.875      3.191      2.242         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:40<00:00,  7.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.204      0.159     0.0879     0.0414\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50      2.77G      1.839      3.095      2.194         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.244       0.18      0.108     0.0508\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50      2.77G      1.808      3.038      2.158          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.322      0.178      0.117     0.0561\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50      2.77G      1.781      2.958      2.127         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.261      0.214      0.138     0.0673\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50      2.77G      1.767      2.913      2.101         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.295      0.213      0.155     0.0777\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50      2.77G      1.747      2.863      2.076         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.245      0.216      0.157     0.0782\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50      2.77G      1.708      2.809      2.059          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.349      0.233      0.179     0.0927\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50      2.77G      1.702      2.777      2.036         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.363      0.239      0.191     0.0965\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50      2.77G      1.667       2.73      2.008         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.312      0.249      0.197      0.103\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50      2.77G      1.674      2.688      2.008         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.298      0.261      0.214       0.11\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50      2.77G      1.649      2.654      1.984         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.295      0.267      0.215      0.114\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50      2.77G      1.648      2.632      1.968         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.314      0.284       0.23      0.123\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50      2.77G      1.624      2.579      1.956         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.337      0.276      0.229      0.122\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50      2.77G      1.614      2.554      1.932          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.323      0.301      0.252      0.136\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50      2.77G      1.595      2.515      1.932         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.355      0.292      0.253      0.137\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50      2.77G      1.597      2.508      1.924         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.381      0.307      0.273      0.149\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50      2.77G      1.588      2.481      1.916         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.354      0.309      0.269      0.148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50      2.77G      1.587      2.466      1.913         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.38      0.315      0.282      0.155\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50      2.77G      1.572      2.437       1.89         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.373       0.32      0.288       0.16\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50      2.77G      1.561      2.413      1.879         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.396      0.306       0.29      0.162\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50      2.77G      1.557      2.381      1.877         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.389      0.331      0.302       0.17\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50      2.77G      1.546      2.364      1.874         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.41      0.333      0.317      0.178\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50      2.77G      1.526      2.348      1.859         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.405      0.338      0.314      0.177\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50      2.77G      1.532      2.333      1.857         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.394      0.342      0.315      0.178\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50      2.77G       1.52      2.324      1.848         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.394      0.346      0.319      0.181\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50      2.77G      1.527      2.307      1.851         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.407      0.354      0.331      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50      2.77G      1.509       2.28      1.832         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.41      0.341      0.331       0.19\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50      2.77G      1.508      2.273      1.827         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.413      0.357      0.336      0.192\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50      2.77G      1.505      2.259      1.824         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.424      0.347      0.341      0.197\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50      2.91G      1.489      2.249      1.816          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.43       0.36      0.344      0.198\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50      2.91G      1.492      2.213      1.816         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.43      0.366      0.355      0.207\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50      2.91G      1.483      2.201      1.799         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.418      0.365      0.354      0.205\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50      2.91G       1.48      2.197        1.8         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.426       0.37       0.36       0.21\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50      2.91G      1.498       2.12      1.846          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.443      0.368      0.362      0.213\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50      2.91G      1.486      2.041      1.831          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.462      0.373      0.369      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50      2.91G      1.473      2.014      1.823          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.459      0.378      0.372      0.219\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50      2.91G      1.456      1.984      1.808         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:40<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.462       0.38      0.378      0.222\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50      2.91G      1.456      1.975      1.802         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.461      0.388      0.382      0.226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50      2.91G      1.448      1.964      1.803         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.477      0.381      0.387       0.23\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50      2.91G      1.444      1.941      1.789         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.479      0.389      0.392      0.233\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50      2.91G      1.439      1.936      1.789         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:40<00:00,  7.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.48      0.383      0.391      0.232\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50      2.91G      1.437      1.921      1.778          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:36<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.475      0.395      0.393      0.234\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50      2.91G      1.433      1.911      1.784          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.479      0.391      0.394      0.234\n",
      "\n",
      "50 epochs completed in 1.100 hours.\n",
      "Optimizer stripped from yolo_training/yolov8n_custom5/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from yolo_training/yolov8n_custom5/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating yolo_training/yolov8n_custom5/weights/best.pt...\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:48<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.478      0.392      0.394      0.234\n",
      "             aeroplane        205        311      0.536      0.489      0.473      0.279\n",
      "               bicycle        250        389      0.533      0.507      0.522      0.321\n",
      "                  bird        289        576       0.39      0.226      0.212      0.109\n",
      "                  boat        176        393       0.47      0.186      0.219      0.112\n",
      "                bottle        240        657      0.332       0.07     0.0844     0.0384\n",
      "                   bus        183        254      0.565      0.508      0.511      0.378\n",
      "                   car        775       1541      0.663      0.538        0.6      0.415\n",
      "                   cat        332        370       0.43      0.432       0.41      0.245\n",
      "                 chair        545       1374      0.442      0.207      0.234      0.114\n",
      "                   cow        127        329      0.395      0.432      0.345      0.207\n",
      "           diningtable        247        299      0.447      0.431      0.409      0.238\n",
      "                   dog        433        530      0.416      0.349      0.338      0.194\n",
      "                 horse        279        395      0.564       0.61      0.616      0.328\n",
      "             motorbike        233        369      0.583      0.518       0.54      0.341\n",
      "                person       2097       5227       0.62      0.554        0.6      0.327\n",
      "           pottedplant        254        592      0.422      0.117      0.136     0.0548\n",
      "                 sheep         98        311      0.369      0.318      0.302      0.172\n",
      "                  sofa        355        396      0.412      0.359      0.343      0.204\n",
      "                 train        259        302      0.532      0.563      0.574      0.356\n",
      "             tvmonitor        255        361      0.448      0.418      0.408       0.25\n",
      "Speed: 0.3ms preprocess, 1.1ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "Results saved to \u001b[1myolo_training/yolov8n_custom5\u001b[0m\n",
      "From-scratch training results saved to yolo_training/yolov8n_custom5\n",
      "\n",
      "Validating from-scratch model...\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.1Â±0.6 ms, read: 29.1Â±8.2 MB/s, size: 106.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 12: Experiment 3 - Training from Scratch\n",
    "print(\"\\n=== EXPERIMENT 3: TRAINING FROM SCRATCH ===\\n\")\n",
    "\n",
    "model_scratch, results_scratch = train_model(\n",
    "    dataset_yaml=dataset_yaml,\n",
    "    model_size=model_size,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size,\n",
    "    pretrained=False,  # Train from scratch\n",
    "    project_name=project_name\n",
    ")\n",
    "\n",
    "# Get the results directory\n",
    "results_dir_scratch = results_scratch.save_dir\n",
    "print(f\"From-scratch training results saved to {results_dir_scratch}\")\n",
    "\n",
    "# Validate the model\n",
    "print(\"\\nValidating from-scratch model...\")\n",
    "metrics_scratch = validate_model(model_scratch, dataset_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.8Â±0.6 ms, read: 20.3Â±5.4 MB/s, size: 102.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:09<00:00, 518.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:42<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.478      0.389      0.394      0.234\n",
      "             aeroplane        205        311      0.539      0.486      0.472      0.279\n",
      "               bicycle        250        389      0.529      0.506      0.524       0.32\n",
      "                  bird        289        576      0.381      0.221      0.211      0.107\n",
      "                  boat        176        393      0.485      0.186      0.219      0.112\n",
      "                bottle        240        657      0.333       0.07     0.0841      0.038\n",
      "                   bus        183        254      0.566      0.508      0.512      0.378\n",
      "                   car        775       1541      0.663      0.537      0.601      0.415\n",
      "                   cat        332        370      0.433      0.435      0.411      0.244\n",
      "                 chair        545       1374      0.445      0.207      0.234      0.114\n",
      "                   cow        127        329      0.391      0.426      0.347      0.207\n",
      "           diningtable        247        299      0.454      0.431      0.409      0.238\n",
      "                   dog        433        530      0.412      0.347      0.338      0.194\n",
      "                 horse        279        395      0.563      0.608      0.614      0.329\n",
      "             motorbike        233        369       0.58      0.518      0.538       0.34\n",
      "                person       2097       5227      0.622      0.553        0.6      0.327\n",
      "           pottedplant        254        592      0.413      0.115      0.135     0.0547\n",
      "                 sheep         98        311      0.361      0.305      0.304      0.174\n",
      "                  sofa        355        396      0.413      0.356      0.344      0.204\n",
      "                 train        259        302      0.534       0.56      0.572      0.355\n",
      "             tvmonitor        255        361      0.449      0.416      0.408      0.249\n",
      "Speed: 0.3ms preprocess, 1.6ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val6\u001b[0m\n",
      "mAP50: 0.3938\n",
      "mAP50-95: 0.2340\n",
      "\n",
      "Per-class metrics (mAP50):\n",
      "  aeroplane: 0.4725\n",
      "  bicycle: 0.5235\n",
      "  bird: 0.2113\n",
      "  boat: 0.2189\n",
      "  bottle: 0.0841\n",
      "  bus: 0.5117\n",
      "  car: 0.6005\n",
      "  cat: 0.4107\n",
      "  chair: 0.2339\n",
      "  cow: 0.3466\n",
      "  diningtable: 0.4089\n",
      "  dog: 0.3379\n",
      "  horse: 0.6139\n",
      "  motorbike: 0.5385\n",
      "  person: 0.6004\n",
      "  pottedplant: 0.1353\n",
      "  sheep: 0.3040\n",
      "  sofa: 0.3439\n",
      "  train: 0.5724\n",
      "  tvmonitor: 0.4077\n"
     ]
    }
   ],
   "source": [
    "#the above kernel crashed on validation so lets access the best weights and validate them\n",
    "# Cell: Validate the model with the best weights\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Path to the saved weights\n",
    "weights_path = \"yolo_training/yolov8n_custom5/weights/best.pt\"\n",
    "\n",
    "# Load the model\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# Validate the model on the validation dataset\n",
    "results = model.val(data=\"datasets/voc_yolo_converted/voc_yolo.yaml\")\n",
    "\n",
    "# Print validation results\n",
    "print(f\"mAP50: {results.box.map50:.4f}\")\n",
    "print(f\"mAP50-95: {results.box.map:.4f}\")\n",
    "\n",
    "# Print per-class metrics (mAP50)\n",
    "print(\"\\nPer-class metrics (mAP50):\")\n",
    "class_names = model.names\n",
    "for i, ap in enumerate(results.box.ap50):\n",
    "    print(f\"  {class_names[i]}: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPERIMENT 4: TRAINING FROM SCRATCH WITH ENHANCED AUGMENTATION ===\n",
      "\n",
      "Starting scratch training with YOLOv8n with enhanced augmentation for 50 epochs\n",
      "New https://pypi.org/project/ultralytics/8.3.131 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.1, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/voc_yolo.yaml, degrees=20.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.1, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.5, mode=train, model=yolov8n.yaml, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_scratch_augmented, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.001, plots=True, pose=12.0, pretrained=True, profile=False, project=yolo_training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=yolo_training/yolov8n_scratch_augmented, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.2, seed=0, shear=10.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.2, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=20\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
      "YOLOv8n summary: 129 layers, 3,014,748 parameters, 3,014,732 gradients, 8.2 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 3.2Â±0.9 ms, read: 16.4Â±3.2 MB/s, size: 94.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/train/labels... 5011 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5011/5011 [00:10<00:00, 484.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 3.0Â±1.5 ms, read: 16.7Â±4.8 MB/s, size: 86.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:09<00:00, 518.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache\n",
      "Plotting labels to yolo_training/yolov8n_scratch_augmented/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolo_training/yolov8n_scratch_augmented\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      2.35G      3.246      4.632      4.079         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:47<00:00,  6.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:39<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976    0.00325      0.164    0.00256   0.000756\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50       2.4G       2.81      4.208      3.433         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:48<00:00,  6.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:41<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976    0.00243       0.24    0.00354    0.00111\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50      2.57G       2.33      4.026      2.888         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:40<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.362     0.0374    0.00911    0.00288\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50      2.57G      2.209      3.896      2.723         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:42<00:00,  7.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.615     0.0179     0.0238    0.00781\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50      2.57G      2.139      3.806      2.643         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:39<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.624     0.0415     0.0271    0.00891\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50      2.57G      2.085      3.715      2.589         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:40<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.45     0.0385     0.0323     0.0109\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50      2.57G      2.057      3.679      2.556         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:40<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.292      0.062     0.0367     0.0131\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50      2.57G      2.031      3.647      2.539         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:39<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.334     0.0751     0.0475     0.0171\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50      2.57G       2.02      3.586      2.519         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.25     0.0983     0.0567     0.0223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50      2.57G      2.004      3.557      2.503         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:42<00:00,  7.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.294     0.0952     0.0534     0.0191\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50      2.57G      1.995      3.545      2.487         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.327      0.114     0.0622     0.0227\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50      2.57G      1.982      3.502      2.472         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:45<00:00,  6.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:40<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.289      0.119     0.0709     0.0261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50      2.57G      1.969      3.479      2.462         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:45<00:00,  6.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:41<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.257       0.12     0.0679     0.0254\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50      2.57G      1.969      3.447      2.458         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:46<00:00,  6.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:39<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.256       0.14      0.086     0.0314\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50      2.57G       1.96      3.431      2.444         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:45<00:00,  6.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:41<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.273      0.135     0.0927     0.0347\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50      2.57G      1.958      3.408      2.442         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:45<00:00,  6.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:39<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.265      0.145     0.0964     0.0369\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50      2.57G      1.941      3.384      2.427         43        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:42<00:00,  7.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.299      0.165      0.115     0.0455\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50      2.57G      1.934      3.361      2.419         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:42<00:00,  7.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:39<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.293      0.174      0.113     0.0454\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50      2.57G      1.938      3.348      2.413         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:42<00:00,  7.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.297      0.187      0.117     0.0446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50      2.57G       1.93      3.341      2.408         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.31      0.187      0.125     0.0494\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50      2.57G      1.929      3.314      2.402         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.306      0.192      0.134     0.0532\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50      2.57G       1.91        3.3       2.39         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.306      0.197      0.133     0.0508\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50      2.57G      1.916      3.305      2.392         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.317      0.207      0.145     0.0577\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50      2.57G      1.906      3.274      2.379         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.308      0.206      0.144     0.0609\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50      2.57G      1.897      3.264      2.369          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.333      0.213      0.153      0.063\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50      2.57G      1.899      3.234      2.371         48        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:44<00:00,  7.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:39<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.335       0.21      0.159     0.0673\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50      2.57G       1.89      3.229      2.368         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:42<00:00,  7.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.337      0.229      0.165     0.0685\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50      2.57G      1.893      3.223      2.367         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:42<00:00,  7.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.352      0.224      0.178     0.0749\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50      2.57G      1.883      3.227      2.357         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.347      0.234      0.176     0.0687\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50      2.57G      1.871      3.206      2.359         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:42<00:00,  7.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.33      0.241      0.176     0.0718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50      2.57G       1.87      3.193      2.351         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.292      0.244      0.181     0.0761\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50      2.57G      1.871       3.18      2.346         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:42<00:00,  7.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.296      0.235      0.187      0.078\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50      2.57G      1.877      3.171       2.35         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.296      0.237      0.183     0.0781\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50      2.57G      1.867      3.174      2.342         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976       0.35      0.242       0.19     0.0825\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50      2.57G      1.856      3.145      2.334         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.371      0.242      0.199     0.0839\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50      2.57G      1.852      3.142      2.338         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.377      0.242      0.204     0.0865\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50      2.57G      1.859      3.144      2.331         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:39<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.354      0.256      0.206     0.0922\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50      2.57G      1.851      3.114       2.33         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.366      0.255      0.203     0.0872\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50      2.57G      1.844       3.12      2.328         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.358      0.259      0.213     0.0926\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50      2.57G      1.839      3.105      2.323         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:43<00:00,  7.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.356      0.267      0.217     0.0974\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50      2.57G       1.68      2.848      2.254          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.366       0.27      0.232      0.104\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50      2.57G      1.669      2.683       2.24          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.374      0.273      0.235      0.108\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50      2.57G       1.66      2.649      2.234          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:41<00:00,  7.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:43<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.359      0.278      0.245       0.11\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50      2.57G      1.659      2.623      2.223         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.354      0.279      0.247      0.115\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50      2.57G      1.659      2.605      2.219         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.364      0.281      0.256      0.119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50      2.57G      1.651      2.595      2.218         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:38<00:00,  8.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:41<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.363      0.291       0.26      0.126\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50      2.57G      1.648       2.56      2.207         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  8.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:37<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.359      0.294      0.266      0.126\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50      2.57G      1.635      2.544      2.197          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:38<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.351      0.303      0.265      0.127\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50      2.57G      1.636      2.539      2.192          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:39<00:00,  7.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:39<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.368      0.292      0.267      0.127\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50      2.57G      1.637      2.534      2.197          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:40<00:00,  7.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:39<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.379      0.292      0.268      0.129\n",
      "\n",
      "50 epochs completed in 1.201 hours.\n",
      "Optimizer stripped from yolo_training/yolov8n_scratch_augmented/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from yolo_training/yolov8n_scratch_augmented/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating yolo_training/yolov8n_scratch_augmented/weights/best.pt...\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:41<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.379      0.292      0.268      0.129\n",
      "             aeroplane        205        311      0.446      0.363      0.373       0.15\n",
      "               bicycle        250        389      0.354      0.401      0.374      0.191\n",
      "                  bird        289        576      0.305     0.0868      0.107     0.0458\n",
      "                  boat        176        393      0.336     0.0433     0.0755      0.032\n",
      "                bottle        240        657      0.327    0.00304     0.0259    0.00965\n",
      "                   bus        183        254      0.343      0.398      0.317      0.163\n",
      "                   car        775       1541      0.513      0.464      0.486      0.261\n",
      "                   cat        332        370      0.355      0.427      0.329      0.162\n",
      "                 chair        545       1374      0.367      0.102       0.12     0.0492\n",
      "                   cow        127        329      0.285      0.325      0.199      0.098\n",
      "           diningtable        247        299      0.369      0.251      0.222      0.103\n",
      "                   dog        433        530      0.292      0.291       0.22      0.113\n",
      "                 horse        279        395       0.38      0.496      0.418      0.182\n",
      "             motorbike        233        369      0.469      0.444      0.435      0.233\n",
      "                person       2097       5227      0.518      0.497      0.495       0.22\n",
      "           pottedplant        254        592      0.395     0.0166     0.0626     0.0233\n",
      "                 sheep         98        311      0.344      0.286      0.231      0.104\n",
      "                  sofa        355        396      0.346      0.232      0.208      0.105\n",
      "                 train        259        302      0.382      0.401      0.355      0.181\n",
      "             tvmonitor        255        361      0.447      0.316      0.298      0.155\n",
      "Speed: 0.3ms preprocess, 1.0ms inference, 0.0ms loss, 2.7ms postprocess per image\n",
      "Results saved to \u001b[1myolo_training/yolov8n_scratch_augmented\u001b[0m\n",
      "From-scratch training with augmentation results saved to yolo_training/yolov8n_scratch_augmented\n",
      "\n",
      "Validating from-scratch model with augmentation...\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.4Â±0.4 ms, read: 22.5Â±8.9 MB/s, size: 106.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 13: Experiment 4 - Training from Scratch with Enhanced Augmentation\n",
    "print(\"\\n=== EXPERIMENT 4: TRAINING FROM SCRATCH WITH ENHANCED AUGMENTATION ===\\n\")\n",
    "\n",
    "model_scratch_aug, results_scratch_aug = train_model_augmented(\n",
    "    dataset_yaml=dataset_yaml,\n",
    "    model_size=model_size,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size,\n",
    "    pretrained=False,  # Train from scratch\n",
    "    project_name=project_name\n",
    ")\n",
    "\n",
    "# Get the results directory\n",
    "results_dir_scratch_aug = results_scratch_aug.save_dir\n",
    "print(f\"From-scratch training with augmentation results saved to {results_dir_scratch_aug}\")\n",
    "\n",
    "# Validate the model\n",
    "print(\"\\nValidating from-scratch model with augmentation...\")\n",
    "metrics_scratch_aug = validate_model(model_scratch_aug, dataset_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.8Â±1.0 ms, read: 20.9Â±2.9 MB/s, size: 88.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:09<00:00, 515.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:43<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.378      0.292      0.268      0.129\n",
      "             aeroplane        205        311      0.446      0.363      0.374      0.151\n",
      "               bicycle        250        389      0.352      0.401      0.373      0.191\n",
      "                  bird        289        576       0.31     0.0885      0.106     0.0454\n",
      "                  boat        176        393      0.323     0.0433     0.0764      0.032\n",
      "                bottle        240        657      0.329    0.00304     0.0259    0.00967\n",
      "                   bus        183        254      0.341      0.398      0.317      0.163\n",
      "                   car        775       1541      0.512      0.464      0.486      0.262\n",
      "                   cat        332        370      0.355      0.424      0.329      0.162\n",
      "                 chair        545       1374      0.369      0.102       0.12     0.0493\n",
      "                   cow        127        329      0.283      0.322      0.198     0.0977\n",
      "           diningtable        247        299      0.367      0.252       0.22      0.102\n",
      "                   dog        433        530      0.291       0.29      0.222      0.114\n",
      "                 horse        279        395      0.384      0.501      0.424      0.183\n",
      "             motorbike        233        369      0.473      0.444      0.435      0.235\n",
      "                person       2097       5227      0.518      0.498      0.494      0.219\n",
      "           pottedplant        254        592      0.397     0.0167     0.0619     0.0232\n",
      "                 sheep         98        311      0.344      0.288      0.233      0.105\n",
      "                  sofa        355        396      0.343       0.23      0.207      0.105\n",
      "                 train        259        302      0.381      0.394      0.356      0.182\n",
      "             tvmonitor        255        361      0.446      0.316      0.299      0.154\n",
      "Speed: 0.3ms preprocess, 1.6ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val7\u001b[0m\n",
      "mAP50: 0.2679\n",
      "mAP50-95: 0.1292\n",
      "\n",
      "Per-class metrics (mAP50):\n",
      "  aeroplane: 0.3738\n",
      "  bicycle: 0.3732\n",
      "  bird: 0.1058\n",
      "  boat: 0.0764\n",
      "  bottle: 0.0259\n",
      "  bus: 0.3167\n",
      "  car: 0.4859\n",
      "  cat: 0.3293\n",
      "  chair: 0.1203\n",
      "  cow: 0.1979\n",
      "  diningtable: 0.2202\n",
      "  dog: 0.2217\n",
      "  horse: 0.4244\n",
      "  motorbike: 0.4355\n",
      "  person: 0.4944\n",
      "  pottedplant: 0.0619\n",
      "  sheep: 0.2334\n",
      "  sofa: 0.2069\n",
      "  train: 0.3562\n",
      "  tvmonitor: 0.2985\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#the above kernel crashed on validation so lets access the best weights and validate them\n",
    "# Cell: Validate the model with the best weights\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Path to the saved weights\n",
    "weights_path = \"yolo_training/yolov8n_scratch_augmented/weights/best.pt\"\n",
    "\n",
    "# Load the model\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# Validate the model on the validation dataset\n",
    "results = model.val(data=\"datasets/voc_yolo_converted/voc_yolo.yaml\")\n",
    "\n",
    "# Print validation results\n",
    "print(f\"mAP50: {results.box.map50:.4f}\")\n",
    "print(f\"mAP50-95: {results.box.map:.4f}\")\n",
    "\n",
    "# Print per-class metrics (mAP50)\n",
    "print(\"\\nPer-class metrics (mAP50):\")\n",
    "class_names = model.names\n",
    "for i, ap in enumerate(results.box.ap50):\n",
    "    print(f\"  {class_names[i]}: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 14: Results Comparison and Visualization\n",
    "# print(\"\\n=== COMPARING RESULTS ACROSS ALL EXPERIMENTS ===\\n\")\n",
    "\n",
    "# # Create a comparison table of results\n",
    "# results_table = {\n",
    "#     \"Experiment\": [\"Transfer Learning\", \"Transfer Learning w/ Aug\", \"From Scratch\", \"From Scratch w/ Aug\"],\n",
    "#     \"mAP50-95\": [metrics_tl.box.map, metrics_tl_aug.box.map, metrics_scratch.box.map, metrics_scratch_aug.box.map],\n",
    "#     \"mAP50\": [metrics_tl.box.map50, metrics_tl_aug.box.map50, metrics_scratch.box.map50, metrics_scratch_aug.box.map50],\n",
    "#     \"Precision\": [metrics_tl.box.mp, metrics_tl_aug.box.mp, metrics_scratch.box.mp, metrics_scratch_aug.box.mp],\n",
    "#     \"Recall\": [metrics_tl.box.mr, metrics_tl_aug.box.mr, metrics_scratch.box.mr, metrics_scratch_aug.box.mr]\n",
    "# }\n",
    "\n",
    "# # Display the table\n",
    "# import pandas as pd\n",
    "# results_df = pd.DataFrame(results_table)\n",
    "# print(results_df.to_string(index=False))\n",
    "\n",
    "# # Plot training results for all experiments\n",
    "# plt.figure(figsize=(15, 10))\n",
    "\n",
    "# # Function to plot results with a specific experiment name\n",
    "# def plot_exp_results(results_dir, exp_name, color):\n",
    "#     results_file = Path(results_dir) / \"results.csv\"\n",
    "#     if results_file.exists():\n",
    "#         df = pd.read_csv(results_file)\n",
    "#         plt.plot(df['epoch'], df['metrics/mAP50(B)'], label=f\"{exp_name} - mAP50\", linestyle='-', color=color)\n",
    "#         plt.plot(df['epoch'], df['metrics/mAP50-95(B)'], label=f\"{exp_name} - mAP50-95\", linestyle='--', color=color)\n",
    "\n",
    "# # Plot each experiment\n",
    "# plot_exp_results(results_dir_tl, \"Transfer Learning\", \"blue\")\n",
    "# plot_exp_results(results_dir_tl_aug, \"Transfer Learning w/ Aug\", \"green\")\n",
    "# plot_exp_results(results_dir_scratch, \"From Scratch\", \"red\")\n",
    "# plot_exp_results(results_dir_scratch_aug, \"From Scratch w/ Aug\", \"purple\")\n",
    "\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"mAP\")\n",
    "# plt.title(\"Training Progress Comparison\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(\"experiment_comparison.png\")\n",
    "# plt.show()\n",
    "\n",
    "# print(\"\\nExperiment comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Project directories\n",
    "project_dir = \"yolo_training\"\n",
    "experiments = {\n",
    "    \"Transfer Learning\": \"yolov8n_custom3\",  # Update these to match your actual folder names\n",
    "    \"Transfer Learning w/ Aug\": \"yolov8n_pretrained_augmented4\",\n",
    "    \"From Scratch\": \"yolov8n_custom5\",  # Update with your actual folder names\n",
    "    \"From Scratch w/ Aug\": \"yolov8n_scratch_augmented\"  # Update with your actual folder names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Function to load metrics from saved model weights\n",
    "def get_metrics_from_saved_model(experiment_name, folder_name):\n",
    "    model_path = os.path.join(project_dir, folder_name, \"weights\", \"best.pt\")\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        model = YOLO(model_path)\n",
    "        \n",
    "        # Validate the model\n",
    "        results = model.val(data=\"datasets/voc_yolo_converted/voc_yolo.yaml\")\n",
    "        \n",
    "        return {\n",
    "            \"Experiment\": experiment_name,\n",
    "            \"mAP50-95\": results.box.map,\n",
    "            \"mAP50\": results.box.map50,\n",
    "            \"Precision\": results.box.mp,\n",
    "            \"Recall\": results.box.mr\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Warning: Model not found at {model_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from yolo_training/yolov8n_custom3/weights/best.pt\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.9Â±0.7 ms, read: 17.7Â±6.9 MB/s, size: 84.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:43<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.793      0.663      0.745      0.524\n",
      "             aeroplane        205        311      0.815       0.72      0.811      0.561\n",
      "               bicycle        250        389      0.826      0.721      0.808      0.588\n",
      "                  bird        289        576      0.763      0.627      0.705      0.447\n",
      "                  boat        176        393       0.68      0.547      0.614      0.357\n",
      "                bottle        240        657      0.794       0.46      0.557      0.361\n",
      "                   bus        183        254      0.787      0.713      0.795       0.66\n",
      "                   car        775       1541      0.877      0.767      0.859      0.636\n",
      "                   cat        332        370      0.823      0.732      0.807      0.601\n",
      "                 chair        545       1374      0.755      0.456      0.574      0.367\n",
      "                   cow        127        329      0.789      0.748      0.806      0.569\n",
      "           diningtable        247        299      0.798      0.596      0.672      0.484\n",
      "                   dog        433        530      0.796      0.694      0.785      0.572\n",
      "                 horse        279        395      0.901      0.797      0.883      0.651\n",
      "             motorbike        233        369      0.854      0.715      0.839      0.573\n",
      "                person       2097       5227      0.861      0.767      0.855      0.577\n",
      "           pottedplant        254        592      0.684      0.463      0.518      0.291\n",
      "                 sheep         98        311      0.737      0.659      0.709      0.506\n",
      "                  sofa        355        396      0.668       0.63        0.7      0.531\n",
      "                 train        259        302       0.83      0.781      0.844      0.604\n",
      "             tvmonitor        255        361      0.827      0.674      0.749      0.537\n",
      "Speed: 0.3ms preprocess, 1.7ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val16\u001b[0m\n",
      "Loading model from yolo_training/yolov8n_pretrained_augmented4/weights/best.pt\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.3Â±0.8 ms, read: 18.1Â±7.6 MB/s, size: 82.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:41<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.763      0.645      0.717      0.459\n",
      "             aeroplane        205        311      0.838      0.717      0.804      0.479\n",
      "               bicycle        250        389      0.831      0.735      0.805      0.541\n",
      "                  bird        289        576      0.801       0.62      0.703      0.396\n",
      "                  boat        176        393      0.679      0.473      0.534      0.287\n",
      "                bottle        240        657      0.739      0.409      0.529      0.294\n",
      "                   bus        183        254      0.806      0.687      0.773      0.584\n",
      "                   car        775       1541       0.86      0.741      0.836      0.575\n",
      "                   cat        332        370      0.758      0.765      0.807       0.54\n",
      "                 chair        545       1374      0.731      0.399      0.525        0.3\n",
      "                   cow        127        329      0.763      0.714      0.778      0.491\n",
      "           diningtable        247        299      0.723      0.549      0.643       0.41\n",
      "                   dog        433        530      0.762      0.702      0.769      0.509\n",
      "                 horse        279        395      0.867      0.776      0.858      0.579\n",
      "             motorbike        233        369      0.841      0.715      0.813      0.525\n",
      "                person       2097       5227      0.836       0.75      0.841      0.514\n",
      "           pottedplant        254        592      0.593      0.422      0.457      0.227\n",
      "                 sheep         98        311      0.691      0.688      0.721      0.474\n",
      "                  sofa        355        396      0.589      0.644      0.636      0.445\n",
      "                 train        259        302      0.785      0.758      0.782      0.511\n",
      "             tvmonitor        255        361      0.774       0.64      0.726      0.495\n",
      "Speed: 0.3ms preprocess, 1.4ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val17\u001b[0m\n",
      "Loading model from yolo_training/yolov8n_custom5/weights/best.pt\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 3.6Â±0.9 ms, read: 12.6Â±2.4 MB/s, size: 82.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:41<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.478      0.389      0.394      0.234\n",
      "             aeroplane        205        311      0.539      0.486      0.472      0.279\n",
      "               bicycle        250        389      0.529      0.506      0.524       0.32\n",
      "                  bird        289        576      0.381      0.221      0.211      0.107\n",
      "                  boat        176        393      0.485      0.186      0.219      0.112\n",
      "                bottle        240        657      0.333       0.07     0.0841      0.038\n",
      "                   bus        183        254      0.566      0.508      0.512      0.378\n",
      "                   car        775       1541      0.663      0.537      0.601      0.415\n",
      "                   cat        332        370      0.433      0.435      0.411      0.244\n",
      "                 chair        545       1374      0.445      0.207      0.234      0.114\n",
      "                   cow        127        329      0.391      0.426      0.347      0.207\n",
      "           diningtable        247        299      0.454      0.431      0.409      0.238\n",
      "                   dog        433        530      0.412      0.347      0.338      0.194\n",
      "                 horse        279        395      0.563      0.608      0.614      0.329\n",
      "             motorbike        233        369       0.58      0.518      0.538       0.34\n",
      "                person       2097       5227      0.622      0.553        0.6      0.327\n",
      "           pottedplant        254        592      0.413      0.115      0.135     0.0547\n",
      "                 sheep         98        311      0.361      0.305      0.304      0.174\n",
      "                  sofa        355        396      0.413      0.356      0.344      0.204\n",
      "                 train        259        302      0.534       0.56      0.572      0.355\n",
      "             tvmonitor        255        361      0.449      0.416      0.408      0.249\n",
      "Speed: 0.3ms preprocess, 1.4ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val18\u001b[0m\n",
      "Loading model from yolo_training/yolov8n_scratch_augmented/weights/best.pt\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 1.8Â±0.3 ms, read: 22.5Â±10.1 MB/s, size: 75.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:40<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.378      0.292      0.268      0.129\n",
      "             aeroplane        205        311      0.446      0.363      0.374      0.151\n",
      "               bicycle        250        389      0.352      0.401      0.373      0.191\n",
      "                  bird        289        576       0.31     0.0885      0.106     0.0454\n",
      "                  boat        176        393      0.323     0.0433     0.0764      0.032\n",
      "                bottle        240        657      0.329    0.00304     0.0259    0.00967\n",
      "                   bus        183        254      0.341      0.398      0.317      0.163\n",
      "                   car        775       1541      0.512      0.464      0.486      0.262\n",
      "                   cat        332        370      0.355      0.424      0.329      0.162\n",
      "                 chair        545       1374      0.369      0.102       0.12     0.0493\n",
      "                   cow        127        329      0.283      0.322      0.198     0.0977\n",
      "           diningtable        247        299      0.367      0.252       0.22      0.102\n",
      "                   dog        433        530      0.291       0.29      0.222      0.114\n",
      "                 horse        279        395      0.384      0.501      0.424      0.183\n",
      "             motorbike        233        369      0.473      0.444      0.435      0.235\n",
      "                person       2097       5227      0.518      0.498      0.494      0.219\n",
      "           pottedplant        254        592      0.397     0.0167     0.0619     0.0232\n",
      "                 sheep         98        311      0.344      0.288      0.233      0.105\n",
      "                  sofa        355        396      0.343       0.23      0.207      0.105\n",
      "                 train        259        302      0.381      0.394      0.356      0.182\n",
      "             tvmonitor        255        361      0.446      0.316      0.299      0.154\n",
      "Speed: 0.3ms preprocess, 1.4ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val19\u001b[0m\n",
      "\n",
      "=== EXPERIMENTAL RESULTS COMPARISON ===\n",
      "\n",
      "              Experiment  mAP50-95    mAP50  Precision   Recall\n",
      "       Transfer Learning  0.523579 0.744529   0.793319 0.663355\n",
      "Transfer Learning w/ Aug  0.458916 0.717006   0.763285 0.645284\n",
      "            From Scratch  0.234027 0.393831   0.478495 0.389444\n",
      "     From Scratch w/ Aug  0.129204 0.267907   0.378165 0.291942\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Collect results from each experiment\n",
    "results_list = []\n",
    "\n",
    "for exp_name, folder_name in experiments.items():\n",
    "    metrics = get_metrics_from_saved_model(exp_name, folder_name)\n",
    "    if metrics:\n",
    "        results_list.append(metrics)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "if results_list:\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    print(\"\\n=== EXPERIMENTAL RESULTS COMPARISON ===\\n\")\n",
    "    print(results_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No results found. Please check your experiment directory names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training progress for Transfer Learning from yolo_training/yolov8n_custom3/results.csv\n",
      "Loading training progress for Transfer Learning w/ Aug from yolo_training/yolov8n_pretrained_augmented4/results.csv\n",
      "Loading training progress for From Scratch from yolo_training/yolov8n_custom5/results.csv\n",
      "Loading training progress for From Scratch w/ Aug from yolo_training/yolov8n_scratch_augmented/results.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 4: Load and plot training progress for each experiment\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "colors = {\n",
    "    \"Transfer Learning\": \"blue\",\n",
    "    \"Transfer Learning w/ Aug\": \"green\",\n",
    "    \"From Scratch\": \"red\",\n",
    "    \"From Scratch w/ Aug\": \"purple\"\n",
    "}\n",
    "\n",
    "for exp_name, folder_name in experiments.items():\n",
    "    results_file = Path(project_dir) / folder_name / \"results.csv\"\n",
    "    if results_file.exists():\n",
    "        print(f\"Loading training progress for {exp_name} from {results_file}\")\n",
    "        df = pd.read_csv(results_file)\n",
    "        plt.plot(df['epoch'], df['metrics/mAP50(B)'], label=f\"{exp_name} - mAP50\", linestyle='-', color=colors[exp_name])\n",
    "        plt.plot(df['epoch'], df['metrics/mAP50-95(B)'], label=f\"{exp_name} - mAP50-95\", linestyle='--', color=colors[exp_name])\n",
    "    else:\n",
    "        print(f\"Warning: Results CSV not found for {exp_name} at {results_file}\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"mAP\")\n",
    "plt.title(\"Training Progress Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"experiment_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.7Â±0.2 ms, read: 14.2Â±4.0 MB/s, size: 84.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:45<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.793      0.663      0.745      0.524\n",
      "             aeroplane        205        311      0.815       0.72      0.811      0.561\n",
      "               bicycle        250        389      0.826      0.721      0.808      0.588\n",
      "                  bird        289        576      0.763      0.627      0.705      0.447\n",
      "                  boat        176        393       0.68      0.547      0.614      0.357\n",
      "                bottle        240        657      0.794       0.46      0.557      0.361\n",
      "                   bus        183        254      0.787      0.713      0.795       0.66\n",
      "                   car        775       1541      0.877      0.767      0.859      0.636\n",
      "                   cat        332        370      0.823      0.732      0.807      0.601\n",
      "                 chair        545       1374      0.755      0.456      0.574      0.367\n",
      "                   cow        127        329      0.789      0.748      0.806      0.569\n",
      "           diningtable        247        299      0.798      0.596      0.672      0.484\n",
      "                   dog        433        530      0.796      0.694      0.785      0.572\n",
      "                 horse        279        395      0.901      0.797      0.883      0.651\n",
      "             motorbike        233        369      0.854      0.715      0.839      0.573\n",
      "                person       2097       5227      0.861      0.767      0.855      0.577\n",
      "           pottedplant        254        592      0.684      0.463      0.518      0.291\n",
      "                 sheep         98        311      0.737      0.659      0.709      0.506\n",
      "                  sofa        355        396      0.668       0.63        0.7      0.531\n",
      "                 train        259        302       0.83      0.781      0.844      0.604\n",
      "             tvmonitor        255        361      0.827      0.674      0.749      0.537\n",
      "Speed: 0.3ms preprocess, 1.6ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val20\u001b[0m\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 3.1Â±1.1 ms, read: 20.2Â±3.2 MB/s, size: 95.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:42<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.763      0.645      0.717      0.459\n",
      "             aeroplane        205        311      0.838      0.717      0.804      0.479\n",
      "               bicycle        250        389      0.831      0.735      0.805      0.541\n",
      "                  bird        289        576      0.801       0.62      0.703      0.396\n",
      "                  boat        176        393      0.679      0.473      0.534      0.287\n",
      "                bottle        240        657      0.739      0.409      0.529      0.294\n",
      "                   bus        183        254      0.806      0.687      0.773      0.584\n",
      "                   car        775       1541       0.86      0.741      0.836      0.575\n",
      "                   cat        332        370      0.758      0.765      0.807       0.54\n",
      "                 chair        545       1374      0.731      0.399      0.525        0.3\n",
      "                   cow        127        329      0.763      0.714      0.778      0.491\n",
      "           diningtable        247        299      0.723      0.549      0.643       0.41\n",
      "                   dog        433        530      0.762      0.702      0.769      0.509\n",
      "                 horse        279        395      0.867      0.776      0.858      0.579\n",
      "             motorbike        233        369      0.841      0.715      0.813      0.525\n",
      "                person       2097       5227      0.836       0.75      0.841      0.514\n",
      "           pottedplant        254        592      0.593      0.422      0.457      0.227\n",
      "                 sheep         98        311      0.691      0.688      0.721      0.474\n",
      "                  sofa        355        396      0.589      0.644      0.636      0.445\n",
      "                 train        259        302      0.785      0.758      0.782      0.511\n",
      "             tvmonitor        255        361      0.774       0.64      0.726      0.495\n",
      "Speed: 0.3ms preprocess, 1.6ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val21\u001b[0m\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.8Â±1.0 ms, read: 16.1Â±5.6 MB/s, size: 74.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:41<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.478      0.389      0.394      0.234\n",
      "             aeroplane        205        311      0.539      0.486      0.472      0.279\n",
      "               bicycle        250        389      0.529      0.506      0.524       0.32\n",
      "                  bird        289        576      0.381      0.221      0.211      0.107\n",
      "                  boat        176        393      0.485      0.186      0.219      0.112\n",
      "                bottle        240        657      0.333       0.07     0.0841      0.038\n",
      "                   bus        183        254      0.566      0.508      0.512      0.378\n",
      "                   car        775       1541      0.663      0.537      0.601      0.415\n",
      "                   cat        332        370      0.433      0.435      0.411      0.244\n",
      "                 chair        545       1374      0.445      0.207      0.234      0.114\n",
      "                   cow        127        329      0.391      0.426      0.347      0.207\n",
      "           diningtable        247        299      0.454      0.431      0.409      0.238\n",
      "                   dog        433        530      0.412      0.347      0.338      0.194\n",
      "                 horse        279        395      0.563      0.608      0.614      0.329\n",
      "             motorbike        233        369       0.58      0.518      0.538       0.34\n",
      "                person       2097       5227      0.622      0.553        0.6      0.327\n",
      "           pottedplant        254        592      0.413      0.115      0.135     0.0547\n",
      "                 sheep         98        311      0.361      0.305      0.304      0.174\n",
      "                  sofa        355        396      0.413      0.356      0.344      0.204\n",
      "                 train        259        302      0.534       0.56      0.572      0.355\n",
      "             tvmonitor        255        361      0.449      0.416      0.408      0.249\n",
      "Speed: 0.3ms preprocess, 1.4ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val22\u001b[0m\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.17 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.6Â±0.3 ms, read: 20.1Â±5.5 MB/s, size: 99.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/kdb/Documents/msds/springMod2/deepLearning/2d_obj/datasets/voc_yolo_converted/val/labels.cache... 4952 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:42<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      14976      0.378      0.292      0.268      0.129\n",
      "             aeroplane        205        311      0.446      0.363      0.374      0.151\n",
      "               bicycle        250        389      0.352      0.401      0.373      0.191\n",
      "                  bird        289        576       0.31     0.0885      0.106     0.0454\n",
      "                  boat        176        393      0.323     0.0433     0.0764      0.032\n",
      "                bottle        240        657      0.329    0.00304     0.0259    0.00967\n",
      "                   bus        183        254      0.341      0.398      0.317      0.163\n",
      "                   car        775       1541      0.512      0.464      0.486      0.262\n",
      "                   cat        332        370      0.355      0.424      0.329      0.162\n",
      "                 chair        545       1374      0.369      0.102       0.12     0.0493\n",
      "                   cow        127        329      0.283      0.322      0.198     0.0977\n",
      "           diningtable        247        299      0.367      0.252       0.22      0.102\n",
      "                   dog        433        530      0.291       0.29      0.222      0.114\n",
      "                 horse        279        395      0.384      0.501      0.424      0.183\n",
      "             motorbike        233        369      0.473      0.444      0.435      0.235\n",
      "                person       2097       5227      0.518      0.498      0.494      0.219\n",
      "           pottedplant        254        592      0.397     0.0167     0.0619     0.0232\n",
      "                 sheep         98        311      0.344      0.288      0.233      0.105\n",
      "                  sofa        355        396      0.343       0.23      0.207      0.105\n",
      "                 train        259        302      0.381      0.394      0.356      0.182\n",
      "             tvmonitor        255        361      0.446      0.316      0.299      0.154\n",
      "Speed: 0.3ms preprocess, 1.5ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val23\u001b[0m\n",
      "\n",
      "=== CLASS-WISE PERFORMANCE COMPARISON (mAP50) ===\n",
      "\n",
      "             Transfer Learning  Transfer Learning w/ Aug  From Scratch  From Scratch w/ Aug\n",
      "aeroplane             0.811466                  0.803855      0.472455             0.373753\n",
      "bicycle               0.808131                  0.805071      0.523510             0.373203\n",
      "bird                  0.704605                  0.703376      0.211318             0.105753\n",
      "boat                  0.613981                  0.534025      0.218857             0.076396\n",
      "bottle                0.557484                  0.528751      0.084129             0.025871\n",
      "bus                   0.795496                  0.772690      0.511707             0.316696\n",
      "car                   0.858559                  0.836028      0.600543             0.485941\n",
      "cat                   0.806757                  0.806900      0.410719             0.329304\n",
      "chair                 0.573592                  0.524950      0.233927             0.120271\n",
      "cow                   0.805676                  0.777778      0.346620             0.197885\n",
      "diningtable           0.672260                  0.642508      0.408941             0.220164\n",
      "dog                   0.784704                  0.769043      0.337861             0.221743\n",
      "horse                 0.883259                  0.857964      0.613867             0.424393\n",
      "motorbike             0.839057                  0.813077      0.538495             0.435486\n",
      "person                0.855202                  0.841008      0.600427             0.494377\n",
      "pottedplant           0.517715                  0.457479      0.135282             0.061895\n",
      "sheep                 0.709016                  0.721152      0.304021             0.233416\n",
      "sofa                  0.700372                  0.636438      0.343922             0.206907\n",
      "train                 0.844100                  0.781854      0.572374             0.356165\n",
      "tvmonitor             0.749156                  0.726173      0.407655             0.298514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 5: Compare class-wise performance for different experiments\n",
    "def get_classwise_metrics(experiment_name, folder_name):\n",
    "    model_path = os.path.join(project_dir, folder_name, \"weights\", \"best.pt\")\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        model = YOLO(model_path)\n",
    "        results = model.val(data=\"datasets/voc_yolo_converted/voc_yolo.yaml\")\n",
    "        \n",
    "        class_metrics = {}\n",
    "        for i, ap in enumerate(results.box.ap50):\n",
    "            class_metrics[model.names[i]] = ap\n",
    "            \n",
    "        return pd.Series(class_metrics, name=experiment_name)\n",
    "    else:\n",
    "        print(f\"Warning: Model not found at {model_path}\")\n",
    "        return None\n",
    "\n",
    "# Get class-wise results for each available experiment\n",
    "class_results = []\n",
    "for exp_name, folder_name in experiments.items():\n",
    "    class_metrics = get_classwise_metrics(exp_name, folder_name)\n",
    "    if class_metrics is not None:\n",
    "        class_results.append(class_metrics)\n",
    "\n",
    "if class_results:\n",
    "    # Combine into a DataFrame\n",
    "    class_df = pd.concat(class_results, axis=1)\n",
    "    \n",
    "    # Display class-wise comparison\n",
    "    print(\"\\n=== CLASS-WISE PERFORMANCE COMPARISON (mAP50) ===\\n\")\n",
    "    print(class_df.to_string())\n",
    "    \n",
    "    # Visualize class-wise performance\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    class_df.plot(kind='bar', figsize=(15, 8))\n",
    "    plt.title('Class-wise mAP50 Comparison')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('mAP50')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"class_performance_comparison.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No class-wise results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## none of my visuals will show in this notebook; see vissub.ipynb ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Discussion of Results\n",
    "\n",
    "Based on the visualizations in vissub.ipynb, we can observe several key trends and insights across our four experimental approaches to YOLOv8 object detection on the Pascal VOC dataset.\n",
    "\n",
    "### Performance Comparison Between Training Approaches\n",
    "\n",
    "The training progress comparison clearly shows a distinct hierarchy in performance:\n",
    "\n",
    "*   **Transfer Learning (standard)** consistently outperforms all other approaches, achieving the highest mAP50 of approximately 0.74 by the end of training.\n",
    "*   **Transfer Learning with Augmentation** begins with lower performance but steadily improves, reaching approximately 0.72 mAP50 after 50 epochs.\n",
    "*   **Training from Scratch** demonstrates significantly lower performance than both transfer learning approaches, reaching only about 0.40 mAP50 after 50 epochs.\n",
    "*   **Training from Scratch with Augmentation** performs worst overall, struggling to reach 0.27 mAP50 after the same training period.\n",
    "\n",
    "### Impact of Transfer Learning vs. Training from Scratch\n",
    "\n",
    "The performance gap between transfer learning and from-scratch approaches is substantial, with transfer learning models achieving approximately twice the mAP50 scores. This clearly demonstrates the value of leveraging pre-trained weights for object detection tasks, especially when working with relatively small datasets like Pascal VOC.\n",
    "\n",
    "### Effect of Augmentation\n",
    "\n",
    "Interestingly, data augmentation appears to have a negative impact on performance across both initialization strategies within our 50-epoch training window. However, a critical observation from the training loss curves reveals a significant dip in augmented model class loss around epoch 40 across all augmented models. This sharp decrease in loss suggests that the augmented models may be reaching an inflection point where they begin to benefit from the additional training variability.\n",
    "\n",
    "This pattern strongly indicates that augmented models would likely outperform their non-augmented counterparts if training were extended beyond 50 epochs. The augmentation initially makes the learning task more difficult, but may ultimately lead to more robust feature learning and better generalization once the model has had sufficient time to process the more diverse training examples.\n",
    "\n",
    "The delayed convergence of augmented models matches established patterns in deep learning, where regularization techniques (which data augmentation effectively serves as) often slow initial learning but improve final performance. The observed loss pattern suggests we may have stopped training just as the augmented models were beginning to realize their full potential.\n",
    "\n",
    "### PR Curves and Class Performance\n",
    "\n",
    "The Precision-Recall curves reveal significant class-specific differences:\n",
    "\n",
    "*   **Transfer Learning models** show strong performance across many classes, with particular strength in vehicles (car, train), animals (horse, cat), and people.\n",
    "*   **From-scratch models** struggle with most classes, with particularly poor performance on smaller objects like bottles and potted plants.\n",
    "*   Certain classes consistently underperform across all models, notably \"bottle\" (0.555 in the best model), \"chair\" (0.574), and \"potted plant\" (0.516).\n",
    "*   The most reliably detected classes are \"person\" (0.855), \"car\" (0.858), and \"horse\" (0.883).\n",
    "\n",
    "### F1 Curves\n",
    "\n",
    "The F1 curves reinforce these findings, with transfer learning models operating at significantly higher confidence thresholds (optimum around 0.45) compared to from-scratch models (optimum around 0.22), indicating much greater certainty in predictions. The from-scratch models with augmentation show the lowest optimal confidence threshold at just 0.17, suggesting very low confidence in predictions.\n",
    "\n",
    "### Training Loss Analysis\n",
    "\n",
    "The loss curves reveal that transfer learning models begin with much lower loss values, indicating the advantage of starting with pre-trained weights. The class loss for from-scratch models with augmentation remains particularly high throughout training until around epoch 40, where we observe a notable decrease. This pattern is consistent across all augmented models and suggests that with extended training, these models might eventually match or surpass their non-augmented versions as they better incorporate the augmented training examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Conclusion\n",
    "\n",
    "This project successfully demonstrated the implementation of a 2D object detection pipeline using the YOLOv8n model on the Pascal VOC 2007 dataset. We covered the entire workflow, from data selection and thorough preprocessing to a comprehensive experimental evaluation using a 2Ã—2 factorial design.\n",
    "\n",
    "**Key Achievements:**\n",
    "\n",
    "*   Successfully preprocessed the Pascal VOC 2007 dataset into a YOLOv8-compatible format.\n",
    "*   Designed and implemented a systematic approach to compare four training strategies:\n",
    "    1.  Transfer learning (standard augmentation assumed, or specify if it was a distinct \"standard\" set)\n",
    "    2.  Transfer learning with enhanced augmentation\n",
    "    3.  Training from scratch (standard augmentation assumed)\n",
    "    4.  Training from scratch with enhanced augmentation\n",
    "*   Evaluated model performance using standard object detection metrics (mAP, Precision, Recall) across all experimental conditions.\n",
    "*   Analyzed the interaction between initialization strategy (transfer learning vs. from scratch) and data augmentation approach.\n",
    "\n",
    "**Summary of Findings:**\n",
    "\n",
    "*   **Performance Comparison & Impact of Transfer Learning:** Transfer learning is crucial for effective object detection with limited data. The models initialized with pre-trained weights (transfer learning) significantly outperformed those trained from scratch, achieving mAP50 scores nearly twice as high (e.g., standard transfer learning at ~0.74 mAP50 vs. from-scratch at ~0.40 mAP50). This demonstrates that the features learned from larger datasets like COCO transfer effectively to new object detection tasks.\n",
    "*   **Effect of Augmentation & Convergence Analysis:** Data augmentation shows promise but requires extended training. Within our 50-epoch window, augmentation appeared to negatively impact initial performance (e.g., transfer learning with augmentation at ~0.72 mAP50, from-scratch with augmentation at ~0.27 mAP50). However, a significant loss reduction around epoch 40, particularly class loss for augmented models, strongly suggests these models were on the cusp of accelerated improvement. This indicates that augmentation likely provides long-term benefits with sufficient training time as models learn from greater data variability. The initialization strategy (transfer learning vs. from scratch) had a greater impact than augmentation choices within this short training regime.\n",
    "*   **Model Capabilities & Object Characteristics:** Object characteristics influence detection difficulty. Transfer learning models showed strong performance across many classes, especially vehicles, animals, and people. From-scratch models struggled more, particularly with smaller or variably appearing objects like bottles, potted plants, chairs, and sofas, which proved challenging across all models. Larger, distinct objects (person: ~0.855, car: ~0.858, horse: ~0.883 in best models) were more consistently detected.\n",
    "*   **Confidence & F1 Scores:** F1 curves reinforced these findings, with transfer learning models operating at significantly higher optimal confidence thresholds (~0.45) compared to from-scratch models (~0.22), and even lower for from-scratch with augmentation (~0.17), indicating greater certainty in predictions from transfer learning.\n",
    "\n",
    "**Limitations of the Project:**\n",
    "\n",
    "*   **Model Scale:** We experimented only with YOLOv8n (nano), the smallest variant. Larger models would likely yield higher accuracy but require more computational resources.\n",
    "*   **Dataset Size:** While VOC 2007 is a benchmark, it's modest in size compared to datasets like COCO, which may limit performance, especially for models trained from scratch.\n",
    "*   **Hyperparameter Consistency:** To ensure fair comparison, we maintained consistent hyperparameters across experiments rather than optimizing each approach separately.\n",
    "*   **Training Duration:** All models were trained for 50 epochs. This may have particularly disadvantaged the from-scratch and augmented models, which typically require longer training to converge and realize the benefits of regularization or learning from diverse examples.\n",
    "\n",
    "**Future Work & Potential Enhancements:**\n",
    "\n",
    "*   **Extended Training for Augmented Models:** Explore longer training periods (e.g., 100-300 epochs) specifically for models utilizing data augmentation to fully assess their potential, given the promising loss convergence patterns observed.\n",
    "*   **Experiment with Larger YOLOv8 Models:** Test YOLOv8s or YOLOv8m to evaluate the accuracy-resource trade-off on this dataset.\n",
    "*   **Hyperparameter Optimization:** Conduct targeted hyperparameter tuning for each of the four training strategies.\n",
    "*   **Advanced Augmentation & Curriculum Learning:** Investigate more sophisticated augmentation techniques and explore curriculum learning where augmentation difficulty is gradually increased.\n",
    "*   **Broader Dataset Evaluation:** Test the generalizability of these findings on other object detection datasets.\n",
    "*   **Deployment Strategies:** Develop model quantization or pruning for the best-performing model for efficient deployment.\n",
    "\n",
    "Overall, this project provided valuable insights into the interplay between transfer learning and data augmentation for object detection tasks. The factorial experimental design allowed us to systematically evaluate these factors. For practical applications on similar datasets, transfer learning should be strongly preferred. When incorporating augmentation, the training schedule should be extended to fully capitalize on its benefits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
